これを持って、マジナルバーサーが出たんだなっていうふうに考えて少しから取りしないとね、一緒に何か振り返ってくれる何の話をすればいいか考えてないんですけれどもちょっと待ってねこういうときのこういうときの雑談スライドシールえー、トークフィューション本以外のスライド200ページの書は難しくていくこと全部見せられないちょっと流行きな話を流行きな話をしようと思ったけどここで非常に大切にポケモンGOのそれがやめようポケモンGOのデジタルスライドあるんですけどこれは、これはもうしろないなやめてこんな話をしてちょっとこれ作ったのがそんなに細分じゃないので自制化してくれるこれはすごいね最近話をすごいねすごいねすごいね、雑談のスライドが出来上がって物語の作品が出来上がってあの、ある、ある友人かともならないのない物語ってスライドがあるこれちょっと面白いかなあのね随分昔の話なんですけど1993年、私が大学入院を終始家での2年生に行ったんですがあの、フランスの大学に3ヶ月間勉強しに行ってましたカリにある、今もこれある今はちょっと名前変わっちゃった、当時はフォールナショナルステレオとカラコンテレコミュニカステモン英語で言うと何ですかね、スクールオブナショナルスーパーテレコミュニケーションユニバーセティンって大学です日本で言うと電気通信大学みたいな、その名前の大学であるえーと、これはハリの13区っていうハリの街の中心から右下に20分くらい歩いたと思うハリって結構、東京に比べるとそんなに大きな街じゃん大きな街ではあるんだけれど割と歩いて東京ってさすがに新宿から東京まで歩いてるっていうのは割と考えないんですけど、ハリはそんなに大きな街この間、カジになったこのモートルダムキャテグラムちょうど真ん中にありますね西野川の中州の、ステトッキー中州の島のここに、島の海岸下のモートルダム寺になるシャンゼリゼっていうのがそこから左上に伸びてるその先に街線もあるエフェル塔は左下の方、私がいたのは右下の方エフェル塔まで地下鉄で3駅くらい歩いても30分かかんないこの時に、私はこの白い背がきてる赤いですね、あまりちゃんと見えなくていいちゃんと見える感じがするこれ5人で映ってますけど、みんな学生みたいに見えるこの中に1人先生がいます、先生この剣型スの教授がいるんですよ難問ですけど、先生はどれでしょう先生はどれだけの客舎で座っている女性がいて、水色のシャツの人白いのが私、その隣にやっぱり生活してる人だったりちなみに先生はね、右から横にいますこの人が先生ですこの人も先生で一番若かったです当時は俺で奪ったんですけどでももう公衆になってその先生の部屋で退退させてもらってこれがなかなか楽しかったですただ、フランスなので言語はフランス語なんですね彼らが普段生活で使う言語私、その時フランス語、今もそんなに使えないけどあまりフランス語できなかったからフランス語を使うときは英語に切り替えてくれたんですけど他にも何か言えないけどこの人たちが先生と私はただのお客さんで3人が当時、県警署にいた日本でいるところの博士課程の学生だったんですこの左から2人目の水色のシャツを着ている彼が俺、ドイツ人、クリストフという名前のドイツ人私より1年が3つぐらい上だったと思います正確な年齢は今も知らない多分3つぐらいの年上、ドイツ人ドイツ人がフランスの大学で博士号を取るというのはかなり珍しいことらしくて別に今、決して仲が悪いわけじゃないけどやっぱりドイツの人ってプライド高いしフランスの人もそれなりにプライドが高いからドイツ人がフランスに来て、フランス語で勉強して学芸論文を書くのは結構珍しいんですちょっと変わったドイツ人がいましたで、当然私日本人でこういうところに入るとまっさきに仲良くなるなんか不思議ですよね日本人はドイツ人と仲良くなってなんか、全然わからないドイツ人かイタリア人、なぜか仲良くなってアメリカに行った時の私の上司がイタリア人でしたねエミコボッケ、すごく、今でも親しくしてます日族、イタリア人、私らっていうこういう仲間たちと3ヶ月間に話してこれ、もちろん、何思ってる?もちろん、私はここにいたの3ヶ月なんだけどその後もずっと日本に帰ってきてから一緒にいろいろ研究して翌年の6月に、これはフランスの国内の学会なんですけれどもさっき紹介したこの水色のシャツ着てるクリストン、それから私の名前があってこの左下の、これが先生ですね、さっきの若い先生これは私の和瀬さんと知っておいてこういう日本とフランスの大学のこれ実はフランス語で書いてあるフランス語の論文、だからなんとなく読めないこれを、彼ドイツ人、ドイツ人と日本人が下の2人は先生の名前なので連盟になっているわけですけれどもドイツ人と日本人が頑張ってフランス語で論文を書いてフランス語学会で発表した1994年は私は博士課で進んでましたけどその時も一回、シャツに渡って一緒に発表しました楽しい思いですその年の秋に今度は、これは日本で開催された学会でICS、今はインタースケーチって名前変わってますけど同じ内容、今度は英語で書いて私の名前は先頭で、クリストクで向こうの先生、こっちの先生としてこういう発表をしましたこれ横浜で、日本でのまたは国際学会が開催されたので横浜で開催をして、その時にクリストクと先生、日本に来てもらって一緒に発表したんですけれども写真なしで、画数とつけの上のものにつっちゃいましたそのクリストクが、その会議に合わせて日本に来てくれたやつ若い、まだ若い私と、これ私の家でどっかしていますけどね3ヶ月か、彼が本当は日本に開催してくれたそれが話の初めですまだまだ全然、長い長い話だから最初その5年後、1999年、この時はもう私千葉高台に確認したのが前の年なので千葉高台2年目ですまだ30くらいだったと思います丹賀に出学会があって、豚フェストって町で学会があってその帰りに、飛行機の関係で、フランフルトで飛行機乗り換えをしたのでちょっと水面に1日だけで、1日くらいだったので彼らの家に遊びに行きましたさっきのこの写真の頃はまだね、ちょうどこの写真の時にこのクリストクが結婚した直後で、初めての赤ちゃんが生まれた直後だったんです私がいる間に、まだ本当に生まれていたり、かわりづらいの赤ちゃんを奥さんに連れてきてくれるたぶん私がその子を抱いた初めての外国人だったはずなんですけどその5年たってですね、その最初に生まれたイザベルちゃんというのが一番右に写ってるこの可愛い子さんの子でこれは少女、当時5歳で奥さんとクリストクがいて左にはこれ、3歳の少男、ファビアンという名前の人彼らの家で、楽しくこれが1999年ですねそのさらに4年後、2003年に今度は、これがスイスジュネーブ、ジュネーブで合体があってその帰りにまた、いろんなところにいつも寄り道するんですけどスイスもドイツに、電車で2時間くらいで移動できるスイスの合体の帰りに、やっぱり彼らの文体寄り道をしました子供たちもちょっと大きくなってさっきイザベルちゃんが、あの時9歳男の子、7歳、大きくなって彼らに下に、レオンというジナルナのままの3人、兄弟クリストクと私、まだ若いですよ、どこの街だったか、どこか遊びにこの時は1週間くらい彼の家に住めてもらっていろいろ楽しく遊びました2003年、20年前ですね、ちょうど新橋が生まれたのかな、このくらいその2年後ですね、2005年の夏にちょうど今も四角大と交点も住んでいるので毎年1人くらい、こっちから、今年も出たいの学生が1人、出学する予定で付ける四角大生の方に行っての方からフランスの学生が来て、交流をやってるその交流協定を最初に締結する仕事っていうのをやらされてなかなか日本の学生にお手入れできるいきなり手紙で頼むのも、受け入れてくれないので私が頑張って1ヶ月の向こう、滞在してる間にあちらの研究室でいろんな分野の研究室の先生を訪ねて行って、日本から交流学生来たらよろしくいくつかその受け口に15人、15研究室くらい、今少し言っちゃったと思いますけど15研究室くらい、直接向こうの先生に会って話をして日本人の留学生、是非、受け入れてくださいって仕事をしてたんですけど1ヶ月間、ちょうどこっちが夏休みの間にただ8月とかもバカなんですけど、1個とんでもない会場、自由時間が多かったのでこの時はね、右から2番目のうちの上のほうでまだ生まれていいんじゃないかって妻と小名を連れて行きました。大変だったけどちっちゃい赤ちゃんを連れてフランスまでのチョコ機の女の子大変でしたけど、その時に日本経済の街の郊外に広い公園があるんだけどそこにわざわざ結構、いつからフランスまで車で4、5時間かかるはずですけど私がフランスにいるって言ったら遊びに来てくれる大きさも少ないけど、家族で遊びに来てくれるさっきのフリーストップ少女、イザブルちゃんがこの時11歳ですねファビアンが9歳になってさっきのちっちゃなお子の子まだ4歳こういうことがあったしここまで大体半分になりますその後ね、残念ながら10年以上彼らが会うチャンスがなかったんです2005年にここで会ったのを最後にしばらくもたまに毎年1回ぐらいフリースマスカードやりとりしたりとかね電子メールやりとりしたりはしてたんだけどそしたらですね、突然これも結構最近ですね2018年5年前コロナにある場合で落とした前の今年2年前突然メールというかこれは何だろうFacebookのメッセンジャーかなんかで突然ポットメッセージが来て読めますか?読めばわかると思うだっと日本語にするとこんにちは、私はイザベル・イント・ホイザーですイント・ホイザーですイザベルです私はフリースマスカードの娘ですさっきの娘ですさっきのイザベルじゃん突然メッセージが来て最後に我々が会ったのはたぶん10年ぐらい前じゃないか私が10歳の頃だと思いますどうしてこれ書いてるかというと実は今空港に行ってこれから東京に行きます現地のドイツの空港からこのメッセージが来て東京に行くんだけどその後にそこにいます日曜日の午後から月曜日まで東京にいるんでもしかもおしゃれがあるんでですみたいなメッセージが普通で来てました2018年4月14日確かすごい忙しい日でこの次の日もとても長い時間が過ごせそうなかったのですぐに遠所しましたびっくりしました今日に来るならぜひ明日の午後どっかで会いましょう10月ぐらい会議をキャンする人がいますけど正確ね、ウィッカー来てくれるなんか彼女の大学に大学もこの時大学卒業すると友達と卒業旅行で日本に来る何人か一緒にいたんだけどこの松町がどっかのホテルに泊まるという連絡をもらったんですぐに隣のホテルに会いましたさっきのイザブリッジはこんなに大きくなったあとの、たぶん私の計算の話が24うちもテックアップで泊んで子供の家に連れてもう連れてお前がホテルに来たのでして再会を楽しみましたこれが2018年4月のまだ終わりません次の年、これはもうこの名もちょっと前代だけど実はこの時が直近で1月最後に外国に行ったのこの時でこれ以来もう4年ぐらい外国に行けてない今年一度行けそうですけど2019年の秋にアーヘンという街で合体がありましたアーヘンっていうのはちょうどケルン彼ら今ケルンですよねケルンから車で1時間ぐらいのとこだったんでその学会の合間は時間をつけてお父さんクリストフと14年ぶりに再会した結構もうおじさんになってこれを後ろを映すのはケルンの有名な世界遺産の大成功ですクリストフと14年ぶりに再会した夜は家族も呼び出したこれはケルンの街の中の一方にちょっと映り悪いけどこれは奥さんのアーネン教堂音に居るのがイザベル一番長寿実はさっき3人映ってたけど4人目が生まれてもう13歳になってシュリアンっていう一番下の男の子の中学1年生ぐらい可愛いところの方が大好きな人です残念ながらその長男のさっき映ってたファミアンはベルリンで働いてるレオンはミュージアムで大学生になっている彼らいなかったので大体楽しく職業をしましたまだ終わりませんもうすぐだったのでその日彼らの家に泊めてもらって泊めてもらわなかったのか深夜までさっき映ったらこのジュリアンっていう末っこの男のチェロを弾くチェロをちょっと聞かせてくれるその時に色々彼らもドイツ人なんで普段は英語喋らないもう中学生ぐらいの普通にドイツの英語が喋れるその時に彼ら向こうは秋にちょうど学年が変わるじゃないですかこのちょうど直後ですかね長い休みがあるんですそこでパパと一緒にどっか旅行するんだって話をしてくれたんですけどまだ行き先が決まってないそしたら一本入れる冗談だったんです一本パパと一緒に遊びにおいで冗談で言ったつもりだったんですけれども月末にこれはプリスと言ったらメッセージだってちょっと字が小さい無事に日本に帰ったと思いますサプラネスにジュリアンと俺は10月に日本に行くぞってメッセージをもらいましたスケジュールが開催10月14日に東京に着いて2週間くらい日本に旅行する京都、奈良、広島いろんな国行きたいという命令をもらいましたしょうがないから成川に行って10月14日2019ジュリアン君と東京さん、成川に来てくれましたその時のその晩はこれ東京駅で何食べたんだっけ親子供か何かのお店に連れて行った親一番子供の大事で一番妻の子は内緒です何が手を振り動きますこれが最後の素振りですけれども正確な話と俺の大学に来いんですかつかむ前に来てもらいましたちょっと帰るうちのメンバー当時の今もいるメンバーが少し今もいるメンバーのさすがにないか去年の終始2年生が本当におさらないそのぐらいのあれですよね私とクリスと東洋一番左に来てる彼は知っている人いるからドイツ語の須藤先生須藤先生は須藤県の人がいるかもしれないフランス語の木島先生の生活だから限定人で仕事を行う彼は今人工知能の専門家ですドイツのザッポっていうやんのSAPというドイツのマイクロソフトと言われる大きなソフトやカンパナーの研究者をやっているんですけれどもこういう好き愛の友達が1人いますそれだけですけれどもこれで終わりですけれども結構ね、彼みたいに長い好き愛の日本人外国人と言われて大事にしている友人たくさんいるんだけどさすがにここまで縁がある人って少々いないんですよねだいたい日本であって彼らは本人となくて娘が突然日本に遊びに来るとかそういうことが多くて非常に仲の良い友人一家今では本当にしばしば行ったり行ったりなかなか遠いんでそう度々行けないんですそれで写真がないんだけれども実は探せばどっかにあると思うんですけどさっき写ってたこの一番右のこれがクリスの奥さん、アンネって奥さんなんだけど奥さんのアンネはもともと彼女がもともとケルンの出身で今は奥さんの実家の近くに彼ら住んでるんですけどそれもあって彼らのご両親をするにその子たちのおばあちゃんおじいちゃんが近くに住んでるんですそのおじいちゃんおばあちゃんの家にも連れて行ってもらってお父さんは絵を描く仕事をしていた家にいっぱい絵が飾るのを数年前に亡くなったという連絡をもらいました家にたくさん絵が描けてあった一枚ぜひ欲しいみたいな話をしたらそのお父さん亡くなった後で2,3,1枚あげるってわざわざ1本まで送ってくれたんですけどその奥さんの両親が住んでる家の隣の家がかつてのメーキャニストのバックハウスウェルフェルムバックハウスあまり知っている人がいたいと思いますけど昔の戦を守らない頃のメーキャニストですねドイツのメーキャニストバックハウスさん家が隣にあるということで実はそのご両親もあったそのピアニストとすごく信仰があってそれもあって私はそのバックハウスさんの子孫子供かまごかんにも紹介をしてもらったことがありますそれでなんとバックハウスが使っていた本当に超メーキャニストの使っていたピアノがその家に置いてあったんですけどピアノも弾かせてもらいましたどこかにそれも動画撮られてなんか私じゃない人がユーチューブに変な日本人が着ている私のピアノ弾いてたりしてユーチューブにあげて今もどこかで見られると思いますそれが私の名前とか書いてなかったかも探せないと思うということでHMの話に戻りますだから最初から聞いてた人はぜひ小レポートをよろしくお願いします学習の話を全体して最後にこういう謎々を出しましたがみんなが知っている確率は全部足して1になるというのがいいと思いますけれどもサイコロにしても、こういうのを投げるにしてもボールを取り出して色が見たいという問題であっても実は基本的にこれまでの多くの確率というのは理算的事象に対する確率だったはずです方法の数、いくつですか確率統計の頃ってもうすごいガッチリあった人は実は高校でも連続の確率事象と扱っているはずだけれどもおそらく多くの人は高校の時に数学で確率統計でそんなに時間かけてもらえなかったと思うのでその話を聞いてないんじゃないかと思います今日はその話をちょっとします雑談で時間を使っちゃったところのこの方が通じやっていいと思いますこの話をしますけれどもどうして確率の話をするかというと今から話があると思いますがHMの中に出てきた出力確率ってやつねどのスライドでもいいんですがこんなのを使って説明をしたいんですねこの時の話は壺からボールが出てきてそれぞれの壺でどの色のボールも入っているんだけれども出てくるボールの割合がちょっとずつ違いますよというそういう説明をしましたこの例でいうところの壺に入っているボールのそれぞれの例えば1番の壺から赤いのが出てくる確率青いのが出てくる確率それぞれ違いますそれが非常に確率ですねこの例外としてはこれでもパーフェクトなんだけれども音声の場合にこれはボールが観測信号赤とか青とか緑というのが音声の場合はスペクトルの形みたいなものに対応していますよという説明をしたと思うけれどもスペクトルの形って何種類ですかって言われても無限にありますよしかも離散量じゃない連続量今やってもらっている来週締め切りですからディーティマッチングで奪ったデータファイルがメル・キャプスラの特徴料を入れてありますけれども不動小数点数が並んでいてしかも1つのフレームあたり15個の数の組で表現されているだから実際にはそういうパラメーターですよね私はそれもディーティマッチングの説明の時にスライドで見せたと思っているけれども例えばこんなやつねこれが今使ってもらっているデータファイルですこれ1行が1フレーム分析フレーム1つあたりにこういう数字がずらっと並んで1つの何かを表しているだからさっきのツボとボールのモデルって言ってみればこのデータの1行を見てこういう数の並びは赤にしましょうこういう数の並びだったらここは青ですねみたいなことをやっているということなんだけれどもその場合に大体そのボールの赤だと青などどういうふうにそれを分類すればいいのかという問題とそもそも連続量だよね連続的な数値を赤と青とかという理算量に置き換えちゃっていいんです場合によっては置き換えずに確率を動員する方法もあるんじゃないのっていうことが今日の主題です今日の話は結構認識においてはすごく大事なのでこの動画の中で私がどういう説明をしているかこれも去年じゃなくて音と字を作った動画だと思いますのでちょっと流しながら後で補足をしますちょっとこの動画が長いとしています動画をいくつかにわたってHMMのたくれまるコップモデルの話をしてきましたアルゴリズムそれからツボとボールのモデルというのを使った例題についてやってきましたけれどもちょっとそのツボとボールの話が続いてしまったために一体どこが音声認識なんだというのが見えなくなってしまっているかもしれませんので以前僕の話はしたんですが改めてHMMにおける秩序確率それが音声認識、音声の特徴量とどうつながっているかを説明しておきたいと思います2つの種類の確率、繊維確率というのと色々と確率がありましたがそのうちの出力確率Bという記号で表しましたけれどもそれぞれの状態に対して定義されてBの入力は指揮でいうところのXTこれ実際に観測された何らかの値が入るんですが音声の場合はここに時刻Tにおける音響特徴量ベクトリー以前やったようなパワースペクトルだとか曲ストラップ、この後で出てきますけれどもそういうものが入ります観測値、あるいは観測シンボルと言ってもいいんですけれども実際のところは音声シンボルではありませんのでシンボルに無理やりにすることができると思いますが壺とボールのモデルにおけるボール、これが音声ですよと言いましたがちょっとそこの表現には工夫が必要なのかもしれませんボールの色、例題では赤と青というふうにしてしまいましたけれどももちろん音声の種類を表現するのには2種類、3種類じゃ全く不足でしてボールの色が仮に何百何千とあるとそう考えればいろんな種類の音声の特徴がボールの色として表現できるのかもしれないその場合もボールというのはあくまでも理算的な特徴にすぎませんからもうちょっと一般的にはこのXTには観測値、特徴量ベットルが入ると考えてくださいそれで今も言いましたが、よく用いられる特徴量ベットルとしてはですねケプスクラムであるとか、ベルケプスクラム、MFCCという特徴量を以前に紹介したかもしれませんがそういったものだとか、パワーだったり、そういうケプスクラムやパワーの時間変化の特徴デルタ特徴とかデルタデルタ特徴と言ったりしますがそういったものをうまく選択して入力として与えるしたがいまして通常XTに入る特徴量は数十次元程度のベットル量が連続値で同源されたベットル量になるわけですねツボとボールのモデルのボール、赤いボールと言っていたものが実は数十次元からなる連続量の特徴量ベットルに対応しているそういうふうに捉えてくださいそういうわけで、確率の計算、Bという出力確率を計算する際に与えられる入力が理算シンボルであったり、連続的な特徴量、しかもベットルであるということから確率を計算する際にもいろんなやり方が考えられるどれが正しい間違っているではなくて、様々な流儀がありますこの後で少しその話をしたいと思いますそれでここでですね、確率についてある程度皆さん知識があると思いますけれども念のためにちょっと理算的な確率と連続的な確率についておさらいをしておきたいと思いますおそらく皆さんがこれまでに数学の授業とかで取り扱ってきた確率のかなり多くの部分は理算的事象に対する確率だったりじゃないかと思います多くの部分というか、要するに小学校、中学校の頃から触れてきた確率というのは多分理算的事象に対する確率ですから、確率分布という意味では、これは後で出てきますが理算確率分布に対応する確率ということになりますが例えばコインを投げるとか、サイコロ振るとかこの間の英大でやったツボからボールを出すなという問題も事象としては理算的であるもうちょっと数学的に言うと、起こり得る事象の数が有限種類ですねサイコロ振るという場合も典型的な場合は、6つの面があって、その6つのいずれかが出るそれでもちろんその際に全ての面が等確率かどうかというのは別に違ってもいいと思いますけれども確率論的には、起こり得る事象の全ての確率を足し算すると必ず1になりますよ確率が1ということは、要するに起こり得る全ての現象がそこに含まれているということですねですから、理算的事象の場合は、たとえどんなに複雑な形をしていてもあるいはコインとかサイコロが仮に歪な形をしていたとしても起こり得る事象の全てを足すと確率が1になる、そういうふうに定義がされますそれでは連続的事象に対する確率はどうなのかもちろん連続的な事象というのがたくさんありますので、それに対しても確率をうまい前に定義しなければいけないのですがこういうふうに考えてみてください例えばサイコロというのは、実は正六面体というんですかね、立方体と言ってもいいんですけれども全ての面が正方形の立方体を使うことが多いですけれども正乱面体というのは正六面体よりもう少し大きな数でも作ることができて、例えば正二十面体なんかありますよねそういうサイコロを作ったとすると、もしもう全ての面が同確率で出るならば、ある面が出る確率は20分の1ということになりますもし正乱とか面体にこだわらなくても良ければ、すなわち面の形が全部違っても良ければ、いくらでも面を増やしていけるその際にどんどんどんどん面の数を増やしていって、もしも無限の面があるようなサイコロを作った場合に、そのサイコロってどういう形になるだろうか同確率ということを考えるならば、それはおそらく9ですよね、ボールみたいな形になると仮にボールをサイコロと見立ててそれを投げる、このこの転がって最後どこかで止まりますね止まった時に一番上に来ている面、これも面というか、限りなく面積が0に小さな面点としてしまうとちょっと解析的におかしくなりますが、限りなく面積が小さい面が上になると考えるならば、その面が出る確率はどうなるかというと無限の面があるということで、6面のサイコロと同じ流儀だとすると無限大分の1ですね無限大分の1というのは皆さん知っているように、限りなく0に小さな数であるはずなんですけれども一つ大事なのは、その無限大分の1の確率が全部合わさると、つまり起こり得るあらゆる確率を全て足し算すると1にならなければいけない確率というのはそういうふうに定義されるものですから、連続的自称であっても起こり得る自称の数が無限小、かつ全ての確率を足し算して1にしなければいけないここで微分積分と同じで、解析的に無限大に近づく、あるいは確率が0に近づくという考え方で、自称に対しては有限小ではありませんので、足し算して1というのはちょっと数学的には難しいということで、積分値が1というふうに考えましょうそういうことで、いろんな連続的な確率の表現があるんですが、後でやりますけれど、例えば正規分布の場合は、全ての定義、マイナス無限大から無限大まで定義が決まりますので、その間の積分値が1になる、そういうふうに定義されます音声のスペプトルパターンとか、テプストラムとかそういうものももちろん連続的自称、もともとは連続的自称なので、この考え方を当てはめる必要があるんですね赤字で、理算確率分布に対応づけて言葉が入れてあります。これちょっと言葉、余分な単語が入っているので、なんか違うように思うかもしれませんが、連続自称のときは、確率分布といっても理算のときと同じようには考えられないので、確率密度という言葉を使うということで、厳密には連続自称を表現する確率分布は、連続確率密度分布と言ったりします。したがって、確率と確率密度というのはそもそも定義の仕方が異なりますから、これ気をつけたいのはごちゃ混ぜにはできません。最後に、確率と確率密度をそのまま足し算するとおかしなことになります。もちろん適当な重みをつけるとか、完全に独立とみなして、別々のパラメーカとして扱う風には混ぜることも不可能ではないのですが、理算的な確率と連続的な確率というのは、そもそも元々の定義が違うということは気をつけていてください。質問を受けた方々の質問です。説明された方々ですが、連続量、今我々が扱うセンサーから取り組むようなデータというのは、結構連続量ですよ。表か裏かみたいな、そういうのが例題としてはもちろんわかりやすいんだ、テレコン。画像にしろ、音声にしろ、センシングデータ、大体連続量です。何に使うかいろいろですが、識別問題を問うという場合も、入力されたデータが、音声の場合はアナのかイナのかを識別したい。これもですね、もう一回以前のスライドを見せします。こういう話をしたと思いますが、この中で音声認識というのは、音声を分析して、何らかのパラメーカーXみたいなのが出てきたときに、このXがそもそもこの和射が喋ろうとした何とかという単語に、どういう関係になっているのかが、ちゃんとうまくつながればそれが認識ができたということなんですが、確率で表現すると、この一番下に指揮があるように、この条件的確率点、Xという辞書が観測されたとき、つまりXが条件になったときに、それがWである確率。Wである確率というのは、IUAOだとしたら、今観測された音声波形が、Aの確率が何%ですか、Iの確率が何%ですか、IUAO全部について確率値を出して、認識問題というのは、その一番高い確率を与えるものが正解ですよというふうに言えばいいんですが、これ難しいのは、この時もちょっと説明しただけで、この条件に入る場合で、左側のWはこれは人間が定義すればいい話だから、音素にしろ単語にしろ無限ってことはない、もちろん数がすごく多くなるかもしれないけど、単語の数も一応この認識システムは、1万単語は使うことができますように決めてあげれば、それでも有限この.wに落とし込めます。だけど、Xの方はどう考えるのか、これ無限ありますよ。あかんわけ、いくらでも作れるし、しかも連続量で。だから実はこれ具合が悪くて、このまま計算したくても、あらかじめすべてのXについて調べておきますということができない。それでこの時の説明ではしょうがないから、これひっくり返しましょう。XとWひっくり返すと後ろに飛び送るのがついちゃうけれども、これならば計算ができるよ。だけど、この時に計算ができるXの確率というのが、今度は条件じゃなくなるから、その確率を与えるそのものです。こっちが無限パターンあるということは、連続量に対する確率をなんとして計算しなくてはいけない。それはどうすればいいんですかというのはさっきの話です。今の話は実はここをどうやろうか。今までは簡単のためにXが赤と青と緑で、例えば小というふうにやっちゃったから、非常にシンプルにできたんだというと、実際にはそうはやらない。それで順序的にどちらを先に扱うのが適切か、実は難しいんですけれども、慣れているということで、理算確率分布で音声認識をするということを考えたい。スポットボールのモデルのボールも理算的な現象ですから、ここではXT入力となる音声の特徴量をわざわざシンボルとして表現すればいいんじゃないですかということですね。シンボルというのは、要するにスポットボールのボールの色に対応するような何か、ラベルを音声の特徴量に振ってあげればいい。そういうことで、色が何色やるかはともかくとして、あるシンボル、赤とか青とか、そういうボールのシンボル表現が、それぞれ特徴量をゲットるの性質に対応していると考えてください。実際にはいろいろやり方があるんですけれども、理算確率分布でパターン認識やるときに、最も典型的に使われる方法を一つ紹介したいと思います。少々して、この後で出てくるベクトル量子化という言葉を使うんですが、手順としてはまず、ベクトル量子化のための前準備として、クラスタリングという処理が行われます。最近はクラスターという言葉は、感染者の集団を表すらしいですけれども、クラスターというのは単に集合とか集団という意味しかありませんので、感染とかそういう意味は含んでないですが、いろんな人がクラスターとか言うとちょっと警戒されてしまうかもしれませんけれども、統計的には、あるいは統計学的には昔からクラスタリング、あるいはクラスター分析という言葉もありますけれども、集合、あるいは集団に対して処理をする、あるいはたくさんのデータを集合にまとめるという意味でクラスターという言葉が使われます。具体的な手順が書いてありますが、まずたくさんのデータが必要である。音声の場合はたくさんの特徴量、ベクトル、ケプストラのようなものをたくさん集める。1つや2つではあまり意味をなしませんが、たくさんのデータを集めます。そのデータに対して、ここちょっと大事なんですが、データの分布に基づいて、スカラ量だと割とヒストグラム作るのが簡単なんですが、ヒトル量の場合は、すべての空間を対象にしようとすると、おそらくほとんどの場所にはデータが一つもないということになりますので、あらかじめデータ分布を見た上で、適切な設定数の部分空間に分けるということが行われます。いろいろと有名なアルゴリズムがあるんですが、例えばここにちょっと書きましたけれども、軽平均法がよく知られているアルゴリズムですけれども、分空間に分けた上でそれぞれにシンボルを付与する。ここでもシンボルは別にボールの色の赤とか青でもいいですけれども、実際にはコードというんですかね、1番2番3番というふうな番号を振っていくことが多いですね。コンピュータでは使いやすいということでね。ただそのときの仮に番号を振ったとしても、1番2番3番の数字には代償関係のような意味はありません。単に背番号のような意味しか持っていないので、2番のコードが1番の何かの量が2倍であるということはありません。ですからシンボルというのは、数字でもいいしアルファベットでもいいと思いますし、赤や黄色みたいな色でもいいと思いますけれども、そういうコードに置き換える。しかたって各部分空間、1つの部分空間が何番というコードで代表的に表現されることになりますが、実際にはその部分空間にはデータがたくさんあるわけですから、クラスタリウムの結果として保持するべき情報としては、その部分空間を代表する点、これをセントロイドと言ったりしますけれども、代表点の座標、複数次元、場合によっては何十次元もの空間にデータが散らばっているわけですので、その代表点の座標を決めて、コードフックと呼ばれるデータ別のものに格納しておくと。陽性意識の場合もこういう手順でやろうとすると、クラスタリウムの結果、コードフックには、大体何百何千という部分空間の代表点が集まることになりますから、その一つ一つが特徴量別の次元数を持っているということで、相当複雑な規模の大きなコードフックが出来上がります。このように準備をしておいた上で、したがってクラスタリウムというのは、ある意味、学習のプロセスと言ってもいいのかもしれませんが、実際のデータに対しては、手順2として、ベクトル量子化、VQと言われたりします、Zector Quantizationの略ですね。量子化という言葉は、いわゆるコンパクトリスクに音を格納するときの量子化と同じでして、連続的な量を理算的な表現に置き換えることですけれども、それが音のサンプリングとは違って、ベクトルになっている。しかも、その入力されるベクトルが、場合によっては何十次元、何百次元ということですので、それを量子化するということになりますが、先ほど作ったコードフックを使って、入力されたベクトルに対しては、最も近いセントロイドを探しましょう。学習のときに、その大量のデータを使って、部分空間を最適に分割してあるので、一番近いセントロイドを探す。そうやってして、そのセントロイドが採用しているコード、シンボルですね。番号だったり、アルファベットだったり、元ボールドモデルの場合は赤、青、そういうようになりますけれども、これを割り当てればいい。手順としては、このVQの手順はそれほど時間がかかりません。なぜかというと、すでにコードフックを持っているわけなので、一番近いのを探すプロセスというのは非常に簡単ですね。もちろん、それに対して準備の段階のクラスタリングは、それなりに時間がかかるプロセスだと思います。ここで言おうとしていることが、今の説明でどのように通じたか分かんないけれども、こういうふうにイメージをすると分かりやすいかなと思います。ベットルじゃなければ、ベットルじゃなくても簡単ではないかもしれないけれども、例えば、英語と数学の点数みたいな例を使うのがよくありがちなんですけれども、もうちょっとインパクトがあるので、例えば人間の身長と体重みたいなものを考えようと。大勢の人がいます。10人、20人だと意味がないけれども、1万人の人がいるとして、その人全員の身長のデータがあるとします。そうすると、多分そのデータの分布というのは正規分布ですよね。平均的な身長の人が一番人数多くて、むちゃくちゃ背が高い人とか、むちゃくちゃ背が低い人は少なくなる。そういう分布だよね。その人たちの集団、1万人の集団を身長によるグループ分けをしましょうと。だから、例えばその1万人を5グループに分けてください。身長の違いによって5グループに分けてくださいとやると、もしも正規分布でなくて満面なく分しているならば、5分割すればいいわけでしょ、その横軸をね。だけど、正規分布みたいな場合は、X軸、横軸に対して5分割するんだと、真ん中のところにも多い人が増えて、両端は少なくなります。だからもし、人数を揃えたければ、平均付近は横軸の幅を狭める必要があるし、そこから離れるところは広くしないと人がたくさん帰らない。スカラリオの場合も、きれいに分割、きれいというのは、大体等分割するためにはそういう工夫をしないといけないというのは、たぶんこれ理解できるでしょう。問題は、それがじゃあ複数次元、ベクトルになった時にどうなるか。横軸が身長、竹軸が体重になる。そういう分布、その偏面に1万人の人のゲートをプロットするとします。つまり、身長がかかって体重が重い人というのは、多分右上の方に行くし、その逆の人が左下の方に。もちろん、身長が高いけど体重が軽い、私以外の人は右下の方に行くかもしれません。だけど、どうですかね。平均的にそういうふうにプロットするとね、おそらく右上がりの再確線上にデータが結構集中しますよね。そういうふうに。イメージ的にね。万年なくその平面に分布することはあり得ない。たまに数率の人もいるかもしれないけれど、身長が低いけど体重が重い人みたいな。そういう人、いっぱいも少ない。数としてはそんなに多くない。その平面を、やっぱり大体人数が同じになるように分割してくださいと言われたらどうしますか。そうすると、やっぱりさっきと同じ考え方で、真ん中付近で。平均子付近の面積は小さくしてあげないと。全部同じ面積にしたら、そこの平均付近にたくさんの人が入っちゃうから、平均付近は面積小さくして、そこから外れるほど面積をちょっと大きめにしないと人数揃わないんです。二次元でも結構、結構めんどくさい感じがしますよね。それをやるために。ちゃんと合理的にやるために。それが、それこそ何十次元になったらどうするんですかという問題です。だから一見、その、そんなの等分割すればいいんじゃないと思うんだけれど、等分割しちゃうと、おそらくそのデータが一個もないような空間が山ほどできてしまって、意味がなくなる。逆にデータがすごい多い空間がいくつかできてしまうというのも識別問題が動くためにはあまり嬉しくないです。そのためにこういうクラスタリングなどの処理をしなきゃいけない。あの説明をもうちょっと続けて。はい、それで今の理算的な処理、クラスタリングをしてからベクトル凝縮化をするというやり方で、能性認識をする場合はどうすればいいかというとですね、書いてありますけれども、あらかじめたくさんのデータについて、VQの結果であるシンボル、世番号のようなものと言いましたけれども、特徴量Xの代わりにVQシンボルCと書きましたが、このCと言語カテゴリー、音素とか単語とかとの関係をあらかじめ調べておけばですね、以前に出てきた条件付き確率、確かあの時はPのW、たてほぼXという表現だったと思いますけれども、このXがCに置き換わることで、このCに対するWの条件付き確率、事語確率が直接計算できます。以前の説明ではこの確率を直接計算できないから、ベースの定理を使って音響確率と言語確率に分解するんですよと説明しましたが、実はベクトル凝縮化を使うとですね、この確率を直接計算することができます。イメージとしては下に簡単な絵が書いてありますけれども、これは二次元平面に対する説明なのであんまり認得ないかもしれませんが、このようにして平面をいくつかの部分空間に分ける、どうしてこんな8モスみたいにしたかというと、これは機械的に分割するならば単純な方式パネルと作ればいいんですが、実際のデータの分布に対して最適な分割の仕方、最適というのはデータの数がですね、統計的に偏らないようにということで、例えばデータが密集しているところはこの部分空間の大きさを狭くする、小さくするとかね、ほとんどデータがない場所は全部まとめてしまうということをする、それが必要になるんですけれども、絵の中でバツで表されているところがセントロイドの座標です。ある空間の代表的な座標、重心みたいなものだと思っていいと思いますが、それに対して例えば赤い丸のようなデータが入ってくると、この一番近いセントロイドということで今線線でつながれているバツが選ばれる。したがってこの赤い丸はその同じ部分空間に属するシンボルが割り当てられるというふうに考えてください。このやり方のメリットは、実はあらかじめPのWカテゴシというのは両方二算的な量ですよね。言語カテゴリもそれからVQのシンボルも有限項ですので、あらかじめすべて関係を調べておけば、実際に認識するときに計算しなくても、その計算された表を見るだけで数字が使える、そういうメリットがあります。そのことをテーブルルックアップと言ったりしますが、あらかじめ確率を計算して表に収めてよ。実際にリアルタイムの処理のときには、そのテーブルを見るだけで確率がそこに入っていますから、計算量を非常に新作できますよね。その反面、デメリットが何かというと、当然今絵に書いてあるように実際のデータは赤い丸なのに、それがすべてこのバツのところのセントロイドで代表されてしまう、置き換えられてしまいますから、この点線のところが誤差になるわけです。それが欠点です。ちょっと補足をしておきますと、この理算確率分布に基づく音声認識、ベクトル量子化を使った音声認識は、今から30年くらい前ですかね、コンピュータの能力がまだ十分じゃなかった頃には結構よく使われていましたけれど、さすがに最近はあんまり見ません。時々非常にコーパクトな音声認識でいいよという場合に、たまに使っているケースを見るときがありますが、ほとんど今はこういう技術はもはや必要なくなったと言ってもいいのかもしれません。私が学生自体はかなりこういうテクニックをよく使った、よくはあります。ですから考え方としては知っておいて、そうはしませんけれども、今こういうやり方を実践の場で使うことはほとんどなくなったと思っていてください。休憩入る前に、さっき出てきたクラスタリングの考え方で、先に身長と体重を比べておかしたけれども、もうちょっとね、実は大学院の授業でたまに話すことがあるトピックで、1時間、2時間くらいかけて調べるスライドの一番最初の例題だけちょっと皆さんにお聞きします。ちょっとあまりわかりやすいスライドじゃない。これ1枚見せられたところでという感じですが、こういう例題、これ非常によく使われるクラスタリングの考え方を説明するのにも非常に便利な例だったんですが、書いてある通りなんだけど、ある街に足すのに家が建っているとします。家が建っているといっても、普通家、本当に雑然と家が建っているような街を想像してください。例えば、遠間隔にきれいに並んでいるような家ならば問題になるかもしれないですよね。普通街の中の家な道だってそんなに真っ直ぐじゃないだろうし、心どころ空き地があったり、ちっちゃな家があったり、木があったり、家があったりということがあると思うので、とにかく、南逆行って家がある街を想像してください。その街にN庫の郵便ポストを設置したいとします。郵便ポストでも電話ボックスもあるんですけど、ここで郵便ポストを設置したい。郵便ポストみたいなのが今、郵便の方を見栄えかされたけれども、重要としては、郵便ポストってそこら中にあるけどね、当然人がたくさんいるところにはたくさん置きたいというそういう思いがあるだろうから、ポストをどこに置きましょうかみたいなことを考えるとあるのが、いろいろ最適な場所みたいなのを考えると思います。だから、郵便ポストを設置する条件として、一つの条件は、ある家に住んでいる人は、最も近いところに設置されたポストを使います。それが条件その一です。逆に、ポストの側から見ると、それぞれのポストは、そのポストを使う人というのは、当然、家から見ると一番近いポストを使う人が決まるよね。街の中でね。何百個の中で、例えば50個くらいの家は、そのポストを使いますというふうに決まったら、そのポストを使う50件の家の、すべての家から最も近い地点、つまり、平均的にということですけれども、ポストをどこに置かないかという問題は、その使う50件の家の中の端っこに置いたんだと、遠くなる人が出てしまったら、真ん中らへん。科学的に言うと、重心付近にポストが置かれると、具合がいいですよね。この条件を満たすように、ポストの場所を決めてください。Nは最初に与えていいと思っております。5個のポストを設置してください。1個のポストの場合は、問題が多少簡単かもしれん。1個のポストを設置するだけならば、すべての家の、代表的、機械学的な、いわゆる重心付近にポストを置けば、そのポストを全員が使わずだから、一番、そのポストから各家までの1人の場が一番小さくなる点というのは、各重心付近ですよね。1個ならば問題は簡単なんだけど、2個以上のポストの場合、5個のポストを置いてくださいとするときに、問題はどうやって解けばいいのですか?問題は一発で決められるのかという問題です。まず、適当にポストを置いてみましょうとやるでしょう。5つのポストを適当に置きます。適当に置いた瞬間に、それぞれの人はどのポストを使うかが決まるよね。ポストの場所が決まるから。だから一番近いポストが見つけられる。そうすると逆に、ポストの側から見ると、そのポストを使う人はそこで決まりますよね。だけどその時に2の条件を満たしているかというと、これは怪しいよ、と言っているような。2番目の条件というのは、そのポストを使う全員から一番近い地点、つまり平均が一番短くなる平均の方が合計でいく。ポストからそのポストを使う家までの距離の合計が一番小さくなる場所に置いてくださいという条件が2番なので、今最初に置いた場所がその条件を満たしていないとすると、ちょっとこっちにずらそうということをやる必要がある。ところが、ちょっとずらしちゃうと、1番の条件が狂いますよ。つまり、最初に置かれたポストが別の場所に移動することによって、それまでAのポストを使っていた理由が、もしかしたらBのポストが近くなるかもしれない。こっちにポストになるうちはこっちに使おう。そうすると、今度また2番の条件が狂いますよ。という具合に、この問題は実は一発では解けない。例題として知られている。この問題の解くためのアルゴリズムというのが、実はさっきちょっと出てきた、軽平均法、軽ミンズアルゴリズム、かなり有名な方法があるんですが、ちょっとここでその説明をするのと、ここでスライドが30枚あるので、やめておきますけれども、考え方としては、ざっと見せましょうか。イメージを掴めればいいと思うんですけれども、例えば、これ、色とかを無視してください。下の枠の中の絵だけ見てね。この丸が家だとします。赤い四角がポストだとします。最初は適当にこうやって赤いポストを置いてあげると、その瞬間に、これ、全部人ですけれども、それぞれのポストを使う家が決まる。道の条件とか何かも含めると、こうやってして、このポストはこの家の人が使います。このポストはこの家の人が使います。ところがそうすると、さっきの話で、例えばこのポストの場合もどう考えても、この丸の人たちが使うと言っても、場所よくない。このポストはもっとこの辺にあるべきです。ということで、こっちに動かす。重心に近くに動かす。そうすると、今度最初に引っ張った線がずれます。わかりますか。このポストの場所を動かすことによって、一番近いポストを使うという条件がちょっとずれますよ。そうすると、またそのポストを使う人の集合が変わるから、またちょっとこれを動かさないといけない。ということを延々と繰り返す必要があります。延々と繰り返すにかけると、徐々にこれは収束に向かいますよ。どこかで止まるか。止まったらその点が一番最適な点ですみたいな、そういうアルゴリズムがあるよというので、この話はちょっと最近あまり時間があってやってないんですけれど、私の大学の授業でしているというこの人たちです。ちなみに、私の卒業論文のテーマでもあったんです。経営分割となっております。これが面白いです。こないだ、エチナムの学習のところで、DLのアルゴリズムが出てきたのが鉛筆を並べるみたいな話をしましたけれども、それとも小観点が一発で決められない問題、しかも複数次元になるということを紹介をしました。ちょっと2、3分休憩しましょう。3分休憩です。3分休憩です。次の質問です。これは実は非常に簡単でして、XTそのまま連続的な入力数として使えますから、テクストラムであるとか、そういった数十次元の特徴量がそのまま確率の変数になるわけですね。ただ、そのときにどんな関数を使うかというところでそのような考え方が当然出てきます。典型的なやり方は、正規分布を使おうと。正規分布ご存知だと思いますが、ノーマルディストリビューションといったり、ガルスヤルディストリビューションといった方がより一般的というのかな、研究者がよく使う表現ですけれども、ガルス分布ってやつですね。もうちょっと具体的には、平均値と分散値で定義されるよくある分布です。他の身の回りのいろんな現象は、正規分布に当てはまることが多くてですね、例えば皆さんにテストをかなりかをやったときの特定とか、水が少ないとそんなにきれいにならないですけれども、多数の人に試験、しかもちゃんと作られたテストをやると、きれいな正規分布になることが知られています。この話は水曜日の方を取ってくれている人は、最初の方で同じ話をしたと、もっとちゃんと正規分布をしたと、ただ数値の2を取っていないというかもしれない方に一応、お話を。あとですね、例えば我々の人間のいろんなサイズ、身長とか体重とかね、これも多くの人を調べると、平均値付近の人が一番多い。そういうことを我々は何となく案に分かっているとも言えますけれどもね、実は音声についても正規分布が当てはまる特徴が多くてですね、もうちょっと具体的に言うと、たくさんの人の声を集めてきて、合わすペクトルなり、テプスカルなりを計算すると、平均的な値というのがペクトルで出てくるわけですけれども、その平均の声というのがどんな声かというのを聞いてみると、ちょっと面白いんですけれどもね、平均値付近の音声の特徴を持っている人が多くて、そこから離れるほど人が少なくなる。直感にも合っていると思います。メンタリー聞かないような声を出す人の数は少ないというそういうことですから、この正規分布が当てはまるのが一番典型的と言ってもいいと思います。ちょっと、僕の話ばかりするとしかなくなっちゃうんですけれども、平均的な声、これ簡単にすくれるんですけれども、聞いてみるとね、なんともつまらない声ですね。だから声ってすごく個性を含んでいるから、これまたね、いい声とか悪い声というのも、何がどうなればいい声なのかって難しいけど、でもあるよね、歌とか聞いて、この人声がいいなみたいなのってありますよ。だけど、真似しようとしてもなかったら難しいです。平均的な声というのは実はすごくつまらないです。同様に、これも有名な話かもしれないけど、人の顔のデータをたくさん詰めてきて、平均的な顔を作る、目の位置とか鼻の位置とか髪の色とか形とか全部調べて、平均的な顔を作るとそれはどういう顔かって、やっぱりね、ちょっと誤辺あるかもしれないけど、つまらない顔です。で、灯台のある顔の研究で有名な先生がある時に、これはあんぶ雑談でた時に、顔のデータをたくさん詰めて、いわゆる、これもどういう語源だったか忘れましたけど、かっこいい人をたくさん集めるとか、かわいい人をたくさん集めるみたいなことであるわけです。美人の顔をたくさん集めてみたいな。で、その美人100人集めて平均にすると、すごい美人ができるのかそうでもなくて、やっぱりつまらない顔になるんです。だから、正規文句になるとは言っても、やっぱり人の特徴っていうか、我々がその人の顔みたいに声を聞いたりした時に、何を捉えているかっていうと、おそらく、みんな同じようなところってあまり気にしないんだろう。しかも、パッと見た時に、誰でも持ってるような、誰でも同じような特徴には目が行かない。だけど、他の人とちょっと違うなってところはすごく気になるんだということですね。目の形にしても、声もそうだよ。だから、おそらくデータとして調べると正規文句になるけれども、人はその正規文句的には、見たり聞いたりしていないように見える。その辺は、非常に面白いやってみるようになる。それで、この辺はどうですかね。皆さん、正規文句はもう、どこかでちゃんと、演奏してるんじゃないかと思っていますけど、あまりこの授業ではしっかり説明しないつもりなんですけれども、ざっとおさらいをしておくと、一次元の正規文句は、こんな式でしたよね。参考手によって、Exponential EXPというのが、これはEの何とかなりのですから、Eの右肩にこの大角を小さく載っている表記の参考手もあると思いますけれども、ちょっと小さくならないためにEXPを使っていますけれども、この式、よく初めて見る人には、欲とされることがあるんですが、実は、円数は右の大角の中に入っている、Xという、これだけが円数で、これ全体はXの関数になってますので、その他の記号、Piはもちろんですし、Xバー、これが平均値ですね、それから分母に2箇所ある、シグマの事情というのは分散値ですね。これらは、正規文句を定義するときには、もうすでに点数として与えられていますから、実は、Xに関する単純な関数と言っていいと思います。右上に、これは多次元で書いてありますけれども、こんな格好した文句が正規文句の定義的な形です。それで、これが多次元になるとどうなるかというと、こういうふうに式が少し複雑になりますけれども、変わったところは、ベクトルになるということで、かけ算の順序がちょっと関係してきますから、右の方のカットの中、XマイナスXバーというところが、Xのベクトル、これが入力の多次元の特徴に対して、Xバーもやはりこれもベクトルですね。それから、シグマ、この式の中にある大文字のシグマは、いわゆる足算の記号のシグマではなくて、共分産業列と言われるものですけれども、共分産業列が2箇所に入っている。あとで、この共分産業列については増えますが、平均ベクトルと共分産業列を使って、こんなふうに多次元の正規文句を表現することもできます。詳細は省略しますけれども、こうやってして求められた確率を使って、1Mを動かすことをすれば、それが連続文句のモデルになるということですね。それで、連続確率密度文句を表すのに正規文句を使う場合ですが、もちろん数学的には、正規文句というのがそんなに何種類もあるわけではないので、先ほどの式列のまま計算すればいいんですけれども、特に多次元になった場合、入力する特徴量の次元数が何十次元になった場合は、この正規文句の式を真面目に計算すると、ちょっと問題が発生する時があります。真面目にという意味で定義式通りの計算のことを、わざわざ全共分産正規文句と呼ぶような人がいますけれども、英語ではフルコバリアンスガーシアンと言ったりするんですが、これは全とかフルという言葉がついていても、数学的に正しく計算するという意味にすぎません。このフルコバリアンスという言葉が持っている意味が、共分産行列というのは、過去に勉強したことある人は分かっていると思いますが、入力するベクトルの次数、もしも十次元のベクトルが入ってきた場合は、ベクトルの次数が10ですけれども、共分産行列はそいつの正方行列ということで、10×10の100個の要素を持っている行列になる。さらに次数が増えた場合、入力データが100次元だとすると、要素が1万になりますように。もちろんその1万になろうが、ちゃんと計算できれば全く問題ないんですけれども、単純に考えて、1万個の要素を持っている共分産行列のパラメータを計算するのに、入力データが1000個しかないなんていう場合も起こり得ます。データの数というのは当然、集められるだけしか使えませんので、定義式というのは、出学において無限のデータが与えられるという条件が暗黙に存在する。ですから実際には、データの数が有限、あるいは少なかった場合には、共分産行列のパラメータがちゃんと計算できないことって結構あるんです。そうした場合の工夫としていくつかバリエーションが存在します。ちょっとそれを紹介してみたいと思います。まず、その全共分産、定義式通りに計算する共分産に対して、対角成分だけを見ましょう。本来は対角成分以外のところに、いわゆる次元間の相関の情報が入ってますから、覗いてしまうのはだいぶ乱暴なんですが、計算量の節約という意味では、対角成分だけを使うというのは、一応多少意味があるのかなと思いますね。これ、ものすごくざっくり言うと、分散についても共分産行列ではなくて、分散ベットルにしてしまいましょう、そういう発想だと思えばいいわけです。したがって、パラメータの数はこれかなり減る。先ほど、例えば100×100の場合は1万と言いましたが、対角成分だけでよければ、100次元の場合も共分産行列は100で済みますので、ですからパラメータの数はかなり減りますが、当然その分表現能力が落ちてしまう。今ちょっと言いましたけれども、次元間の相関の情報が一切見られなくなってしまいますので、このバランスを考えて表現能力を多少打ちても、パラメータ数を減らしたいという場合に使われる方法です。それで今の対角共分産というのは、パラメータの閉算度を減らすのにが最も手つり早い方法なんですが、さすがに表現能力が落ちすぎてしまうということで、接中案というんですかね、全共分産と対角共分産のいいとこ取りをするような考え方として、最もよく使われているのが、この混合正規分布というやつです。英語ではミクスチワーガウスやんと言ったりしますね。このミクスチワーガウス分布をミックスするという意味ですけれども、見かけ上、全共分産正規分布の表現能力を保ったまま、対角共分産とほぼ等々、場着感を下さる派なんですけれども、パラメータ数で表現ができるということになります。考え方としては式に書いてありますけれども、確率密度関数Fxを、いくつかの正規分布の和で表現しよう。一つ一つの正規分布は、例えば対角共分産のシンクルなものであっても、何らかの重み係数でそれらを足し合わせることで、結構複雑な形も表現できますよ、そういうアイデアです。それからもう一つ、これも長年よく使われてきたアイデアでして、研究者によっていろんな表現を使うんですが、私がこういうことに関わっていた頃は、半連続とかいう人が多かったんですけれども、後にちょっとこれ意味が分かりにくいということで、あまり使われなくなりましたけどね、英語の表現である態度ミクスチャーという言い方が、よく知られているかもしれません。態度というのは、結びつけるとかつなぎ合わせという意味で、ミクスチャー、先ほどの混合正規分布を、いくつか結びつけるという、そういう意味を持っているんですけれども、これ、具体的には、先ほどお話しした離散分布、ベクトル量質化を使うやり方の、セントロイドの表現に連続確率密度分布を与えておくと、単にそこにシンボルがあるとするのではなくて、それぞれのセントロイドに正規分布を割り当てておくことで、見かけ上はその正規分布の足し合わせで表現されますよということで、既定分布を共有するという言い方を使うこともありますけれども、そういうふうなアイディアが提案されています。あとで時間があったら、ちょっと詳しく説明したいと思います。はい、それでちょっと言葉だけだと、直感的に理解しにくいかもしれませんので、これ、かなり昔のある解説記事から引用した図なんですが、いろいろな確率分布の表現がどのぐらいの能力を持っているのかを、直感的に理解しやすいかなと思ったので、先ほどの説明に加えて少し紹介してみたいと思います。この絵は多次元ということで、何十次元という話をずっとしてきましたが、何十次元の空間をスライド上に、平面上に描くことはできませんので、この絵は二次元で説明してあります。X、Y平面上、二次元の平面上にデータが分布していたときに、いろいろな確率分布が何をどういうふうに表現するかという概念図のようなものですけれども、まず一番左上のAと書いてあるところ、サンプルディストリビューション、点がいっぱい調ばっていますが、これが実際のデータ分布だとします。空間のような形をして、ちょっと右上がりの特徴を持っている分布だと思えばいいなと思います。この点線で囲んだ砂豆みたいな形を何とかして、上手に表現できればいいんですが、最初の方でやりました理算分布、理算確率分布を使った考え方が、その右にあるBとC、Cについてはちょっとこの授業中は説明していませんので、Bを見てほしいんですが、シングルコードブックと、このデータの分布になるべく忠実に理算分布をクラスタリングによって学習すると、おそらくわざと歪ませてあるのは、多分データの分布をそこまで忠実には学習できないということを表しているようですけれども、データの散らばりを極力忠実にモデル化できると。当然この場合はXY平面上とはいえ、理算分布というのは特に関数で数理的に表すということではないですから、このような唯一な形になるだろうと、そういうふうに理解すればいいわけです。これも一つの考え方としては、元のデータ分布に近づけるという意味では非常に有用と言ってもいいのかもしれません。それに対して下の3つDEFというのが正規分布を使った表現具なんですが、まず真ん中を見てください。古小バリアンスというもので一つの正規分布で表すと、このケースではXとYに多少の相関がある。正規分布の表現の仕方は、2次元平面上で大円が書いてあって、この大円が何を意味するかということですが、分かりやすく考えるならば、平均から離れている距離が近い分散値を一定にしたところをつなぐと。何度もいいんですけれども、1シグマ、標準偏差のところの等しくなるところを結ぶと、こういう大円ができるんですけれども、くすわい平面上で本来正規分布というのは、相関を表す成分がこの大円の傾きに相当しますから、例えばこんなふうに当てはめることができるだろうと。元々の分布が点線で書いてありますけれども、何となく重なるような形が関数、パラメトリックな表現と言ったりもしますけれども、正規分布の式で当てはめることができるわけですね。ところが先ほど言った理由で、今は2次元ですので、共分産業率はそんなに大きなサイズにはならないんですが、考え方として、ダイアゴナル小割合図、対角成分だけにするというのがどういうことかというと、その左側、Dと書いて、このような絵がそれに相当しますけれども、対角成分しか使わないと相関の情報はどうやっても表現できないので、それはこの絵でいうところの大円の軸がX軸Y軸に平行にしかなり得ないということで、どんなに頑張ってもこのぐらいの表現能力しか持たないわけですね。最後にFミクスチャーデンセキと書かれていますが、先ほど説明したミクスチャーガウスヤンに相当する考え方です。この絵では今4つの大円になってますけれども、この4つの大円それぞれは、軸がX軸Y軸に平行である、すなわちダイアゴナルの正規分布と、相関を見ないような正規分布で表されているとして、それを組み合わせることで、このように点線の分布をきれいに表現できる、足し算で表現することはこういうことですけれども、複数の正規分布で一つの大きな点線の役を生まれた分布を表現してあげることができます。この絵では4つになってますけれども、実際には混合する数はもっと多くても構わないので、何十という正規分布を足し合わせてミクチャー表現するということがよく使われているようです。この絵は簡単に伝わりましたが、連続ブロックについては、体格共分散するから自分に全部計算するのの違いというのは、あくまでもこれは概念図なので、この楕円というのはさっきのどれだったか、これだと思ってください。この楕円がありますね、これちょっと変物的に入って多いんだけど、正規分布のある一定の確率を表すところを繋ぐと、これ式で表すわけだから、この多次元の式のある一定のエクチを与えるところが楕円になります。どの場所を振るかはそれぞれなと思っているけれども、その同じところを結んであげると、体格共分散の場合は、この楕円の軸が必ずこのケースは2次元で説明しているので、X軸、Y軸に平行な軸にしかならない。その楕円の広がりとかは変えられますけど、ただもともとこのデータみたいに右上がりに点が違わっているような分布を表すには、ちょっと奥向きですね。もちろんちゃんと全共分散計算できれば、この右上がりというのが表現できるけれど、それはさっき言った理由で、データがすごくたくさんないと、どうしてもその計算、データの数が少ない場合は、共分散の上でちゃんと計算するのも難しいよということで、混合分布みたいな選手案が使われますということです。これは実際に混合分布を使って認識モデルを作ってあげると、性能的には当然ですね、これデータの数によっては、この左のダイヤルになるよりはいくらか良くなりそうな感じがします。元のデータにきっかけ合わさっているということです。で、割とよく使われてきたということです。最近はどうなっているのか?最近はどちらかというと、分布そのものを機械学習でモデル化しようと、いわゆるその整形分布みたいなものを案に仮定しないことが増えてきているので、リアルネットワークなどもまさにノンパラメトリックな分布の代表格です。だから最近のトレンドとしては、混合分布とかそういうところで区分をする必要性も徐々になくなりつつあるのかなと思いますが、やっぱりでもこれ認識の確率の考え方の基礎としても大事な話になって、なんとなくこういうアイデアがあるよというのは、知っておいております。それでもう1個は無理ですね。あとですね、残りはちょっとこのHMMに関するいろんな工夫が行われますよという話と、あとちょっと手前にその話も入れてあるはずですが、私が昔に取り組んできた研究技術課紹介した上で、HMMの話はご覧にしようかなと思います。来週までだったと思います、ギリギリマチューンの課題の締め切りで、さっき一度見たら23人くらいの人が提出してくれているみたいな、中まで見てない方でも頑張って取り組んでくださいなとか言っているように、あの課題に忘れなりの必要を持って、1マス試験相当の評価をしたいと思いますので、忘れなりに頑張ってくれた人にはいい評価が付けられると思います。それで、主体あたりにもいかがですか、今日はまだ誰こと課題の前ですから、今まで言うことではないけれども、皆さんの来週の定数状況の内容を見た上で、その先については考えようと思います。とりあえず、あの課題を頑張ってやってくださいということで、今日はこの辺りを終わりにしております。最初から言ってくれた人は、一人の小レポート課題をお見せしております。しっかりと解説をしておいてもらっております。はい、終わります。何か言わせるな、何があっていると、お気をつけください。はい、終わります。今日のこの6号課題の時間ありますよ。ありますよ。