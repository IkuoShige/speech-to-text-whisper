これをもって、マジナルバンカーが選んだというふうに、何の話をするか。少し後押ししないとね、選択ができない。何の話をすればいいか、考えてないですけれども、ちょっと待ってね。こういう時の雑談スタイル集。東北有書。すごいね、このスタイルを。200ページの書がある。難しくて、全部見せられない。ちょっと流役の話を。流役の話をしようと思ったのに、これにしよう。ポケモンGOの、あ、でもこれはやめよう。ポケモンGOを支える技術がいっぱいあるんですけど、これは面白くないな。やめて、この話にしよう。これ作ったのが、そんなに最高じゃないので、時勢が悪くないかもしれないけど、多分大丈夫。すごい、雑談のスライドが出来上がってきて、物語自作が出来上がってきて。ある友人に書くのが、ないのない物語というスライドがあります。これちょっと面白いから、聞いてください。随分昔の話なんですけれども、1993年、私が大学院を修士課での2年生に届きました。フランスの大学に3ヶ月間、勉強しに行ってました。パリにある、今もこれあるんですけど、今はちょっと名前変わっちゃって。当時は、エステイニューステーション、ホールナショナルステリエール、トラック、テレコミュニカシオン。英語で言うと、スクールオブナショナルスーパーテレコミュニケーション、ユニバーサティ大学です。日本で言うと電気通信大学みたいな名前の大学です。これは、パリの13区という、パリの街の中心から右下に20分くらい歩いたところ。パリって結構、東京に比べるとそんなに大きな街じゃん。大きな街ではあるんだけれど、割と歩いて、東京って流石に新宿から東京駅まで歩いてというのはあまり考えないです。パリはこんなに大きな街。この間火事になったノートルダムキャテドライス大地震が、ちょっと真ん中にありますね。西の側、ナパス、ペトープというナパスの島のところにある。島の海岸下、ノートルダム地になる。シャンゼリーで言うと、そこから左上に伸びて、その先に凱旋門があります。エプテル塔は左下の方、私がいたのは右下の方。エプテル塔まで地下鉄で3駅くらい。歩いても30分かかんないです。マリア。この時に、私はこの白いセーターを着て、赤いです。あまりちゃんと見えなくていい。ちゃんと見えるのは難しいです。これ5人で写ってますけど、みんな学生みんなに見えてる。この中に一人先生がいます。この研究室に教授がいるんです。何問ですけど、先生はどれでしょう。先生どれだと思う。下で座っている女性がいて、水色のシャツの人。広いのが私。その隣にやっぱり背が立っている人がいます。ちなみに先生は右から軽いです。この人が先生です。どう見ても先生一番若くて、当時は結構赤くてこれでいいと思います。でももう高齢者になって、その先生の部屋に滞在させてもらって、なかなか楽しかったです。ただ、フランスなので、彼らが普段生活で使う言語はフランス語なんです。私はその時フランス語は今もそんなに使えないです。あまりフランス語ができなかったから、私と喋るときは英語に切り替えていました。他にも何人かいられる。この人たちが先生と私の方のお客さんで、この3人が当時研究室にいた日本でいるところの博士課程の学生だったんです。左から2人目の水色のシャツ着ている彼がドイツ人。クリストフという名前のドイツ人。私より年が3つぐらい上だったと思います。正確な年齢は今も知らない。多分3つぐらいの年ぐらい。ドイツ人。ドイツ人がフランスの大学で博士の博士をとるのはかなり珍しいことらしくて、今は決して仲が悪いわけではないけど、ドイツの人はプライドが高いし、フランス人もそれなりにプライドが高いから、ドイツ人がフランスに来てフランス語で勉強して学位論文を書くのは結構珍しい。ちょっと変わったドイツ人がいました。当然私が日本人でこういうところに帰ると、まず先に仲良くなる。やっぱり不思議ですよね。日本人がドイツ人と仲良くなって。なんか、戦前のドイツ人とイタリア人がなぜか仲良くなって。アメリカに行ったときの私の上司がイタリア人でしたね。ヘンリコボって。すごく今でも近しく知ってます。日独いとかいう話があって。こういう仲間たちと3ヶ月間話して。これ、もちろん、何と言うの。もちろん私はここにいたの3ヶ月なんだけど、その後もずっと日本に帰ってきてから一緒にいろいろ勉強して、翌年の6月にフランスの国内の学会なんですけれども、さっき紹介したこの水色のシャツ着てる。クリストフ。それから私の名前があって、左下のこれが先生ですね。さっきの若い先生。これは私のワセランとしておいて、こういう日本とフランスの大学の、これ実はフランス語で書いてある。フランス語の論文。だから、なんとなく近い思い出なんです。この、彼ドイツ人。ドイツ人と日本人が、下の2人は先生の名前なので連名になっているわけですけれども、ドイツ人と日本人が頑張ってフランス語で論文を書いて、フランスの学会が発表した。1994年はもう私は博士課程に進んでましたけど、その時もう1回、2ヶ月にわたって一緒に発表しました。楽しい思い。この年の秋に今度は、これは日本で開催された学会で、ICSRB、今はインタースペーシュと名前変わっていますけれども、同じ内容を今度は英語で書いて、私の名前は先頭で、クリストフ、で、向こうの先生、こっちの先生として、こういう発表をしました。これは横浜で、日本でたまたま国際学会が開催されたので、横浜で開催をして、このクリストフと先生が日本に来てもらって、一緒に発表したんですけれども、で、写真は小さい。大きくすると、杖の上のものにつきあがる人がいます。クリストフがその会議に合わせて日本に来てくれました。まだ若い私と、これは私の家で撮った写真ですけれども、3ヶ月間、あれが今度は日本で開催してくれた。それが話の始めです。まだまだ全然、長い長い話だから最初。その5年後、1999年、この時はもう私は千葉高大に職人にしたのが前の年なので、千葉高大2年目です。まだ30くらいだったと思います。タンガリーで学会があって、プタフェストという街で学会があって、その帰りに、飛行機の関係で、フランクフルトで飛行機乗り換えをしたので、ちょっとついでに1日開けて、1日か2日くらいあったので、彼らの家に遊びに行きました。さっきのこの写真のこれはまだです。ちょうどこの写真の時に、このクリストフが結婚した直後で、初めての赤ちゃんが生まれた直後だったんです。私がいる間に、まだ本当に生まれて1ヶ月くらいの赤ちゃんを奥さんが連れてきてくれて、私がその子を抱いた初めての外国人だったはずなんですけど、その5年経って、最初に生まれたインバベルちゃんというのが、一番右に映っているこの可愛い女の子。これは長女、当時5歳。奥さんとクリストフがいて、左側は3歳の長男、ファビアンという名前の彼らの家です。楽しく。これが1999年です。そのさらに4年後、2003年に今度はスイス、ジュネーブで学会があって、その帰りにまた、いろんなところにいつも寄り道するんですけど、スイスもドイツに、電車で2時間くらいで移動できるので、スイスの学会の帰りに、やっぱり彼らと寄り道をしました。子供たちもちょっと大きくなって、さっきのインバベルちゃんがその時9歳。男の子7歳、ちょっと大きくなって、さらに下にレオンという字などが生まれて、3人兄弟。クリストフと私、まだ若い。どこの町なのか、どこか遊びに。この時は1週間くらい彼らの家に泊めてもらって、いろいろ楽しく遊びました。これ2003年です。20年前ですね、ちょうど。君たちが生まれたのこのくらい。その2年後です。2005年の夏に、ちょうど今も千葉高大と教廷が住んでいるので、毎年1人くらいこっちから、今年も出たいの、学生が1人という学生で予定ですけれども、千葉高大生のほうに行って、向こうからフランスの学生が来て、交流をやって、その交流協定を最初に締結する仕事をやらされて、なかなか日本の学生が受け入れてくれる。いきなり手紙で頼んでも受け入れてくれない。私が頑張って1ヶ月間ここを滞在している間に、あちらの研究室でいろんな分野の研究室の先生に訪ねていて、日本からこういう学生が来たらよろしく、受け入れて、15研究室くらい、今少しで言っちゃったと思いますが、15研究室くらい、直接向こうの先生に会って話をして、日本人の留学生はぜひ受け入れてくださいという仕事をしていたんですけれども、1ヶ月間、ちょうどこっちが夏休みの間に、ただ8月とかはもうバカなんですけど、人ほとんどいないので、自由時間が多かったんです。右から2番目のステンプル、うちの上の方で書いてある、まだ生まれていいんだよ。妻と少奈を連れて行きました。肝炎になってるよ。ちっちゃい赤ちゃんを連れてフランスまで飛行機乗るの。肝炎でしたけど。その時に、東京の町の郊外に広い公園があるんだけど、そこにわざわざ、結構ドイツからフランスまで車で小時間かかるはずです。私がフランスにいるときから遊びに来てくれる。大きさも少ないけど、家族で遊びに来てくれました。さっきのクリストフと、少女、リザブルちゃんがこの時11歳ですね。パピオンが9歳になって、さっきのちっちゃな男の子がまだ4歳。こういうことがあったんです。ここまでで大体半分です。その後、残念ながら10年以上、彼らがあるチャンスなかったんです。2005年にここで会ったのが最後に。しばらくも、たまに毎年1回くらい、ベースマスカードやり取りしたり、電子メールやり取りしたりはしてたんだけど、そしたら突然、これも結構最近です。2018年、5年前。これは何かの前年か。前年、2年前。メールというか、これは何かの、Facebookのメッセージだったのか。突然、ホットメッセージが来て、読めますか?読めばわかる。ざっと日本語にすると、こんにちは。私はイザベル・ウィントホイザーです。ウィントホイザーは彼らの名前。イザベルです。私はクリストフの娘です。さっきのイザベルじゃん。突然メッセージが来て。最後に我々が会ったのは、たぶん10年くらい前。私が10歳の頃だと思います。どうしてこれ書いてるかというと、実は今、空港にいて、これから東京に行きます。現地のドイツの空港からこのメッセージが来る。東京に行くんだけど、その時にそこにいます。日曜日の午後から月曜日まで東京にいるので、もしもし会えたら嬉しいです。みたいなメッセージが突然来ました。2018年4月14日。確かね、すごい忙しい日で、この次の日もとても時間を作れそうになかったんです。すぐに返事をしました。びっくりしました。東京に来るなら、ぜひ明日の午後、どこかで会いましょう。2つぐらい会議をキャンセルしたいと思います。せっかくドイツから来てくれる。彼女の大学も、この時大学卒業すると、友達と卒業旅行で一緒に来る。何人か一緒にいたんだ。小松町かどこかのホテルに泊まると連絡もらったので、すぐに都内のホテルで会いました。さっきのイザブリでこんなに大きくなった。たぶん、私の計算が正しいけど、24㎡。うちもせっかくなので子どもたち連れて、もう連れて、都内のホテルで会会を話しました。これが2018年の4月。まだ終わりません。次の年、これはもうコロナのちょっと前代。実はこの時が直近で一番最後に外国人がこの時で、これ以来もう4年くらい外国に行っていない。一年一度行けそうで。2019年の秋に、アーヘンという町で学会がありました。アーヘンというのは、彼らはケルンに住んでいる。ケルンから車で1時間くらいのところだったので、その学会の合間、時間を見つけて、これはクリストフと14年ぶりに再会した。結構おじさんになった。これ後ろを映しているのはケルンの有名な世界遺産の大聖堂です。これはクリストフと14年ぶりに再会した。夜は家族も呼び出した。これはケルンの町の中の神輿で、これは映り悪いけど、これは奥さんです。アンネとオトニンがいた場合は一番近い。実は、さっき3人映っていたけど、4人目の生まれても重産していた。ジュリアンという、一番下の男の子。中学1年生くらいの可愛い男の子が近いです。残念ながら、町内のパピアはベルリンで働いている。レオンはミュージンで大学生になっている。彼らはいなかったので、大体楽しくショッピングをしました。まだ終わりません。その日、彼らの家に泊めてもらった。泊めてもらわなかったのか。深夜まで行く。さっき映ったジュリアンという、末っ子の男の子。チェロを弾く。チェロをちょっと聴かせてくれた。彼らもドイツ人なので、普段は英語を喋らない。もう中学生くらいの時期です。英語が喋れる。その時に彼らが、向こうは秋にちょうど学年が変わる。ちょうど直後ですかね。長い休みがある。そこでパパと一緒にどこか旅行するんだって話をしてくれたのに、まだ行き先が決まってない。しっかり日本に出る。冗談で言ったんです。日本にパパと一緒に遊びにおいで。冗談で言ったつもりだったんですけれども。月末に、これはクリストフからメッセージが来ている。ちょっと字が小さい。無事に日本に帰ったと思います。桜林さん、ジュリアンと俺は10月に日本に行くぞ。メッセージをもらいました。スケジュールが改善。10月14日に東京に着いて。2週間くらい日本に旅行する。京都、奈良、広島、いろんな国に行きたい。スケジュール、平壤ももらいました。しょうがないから、成田は2回目ですね。10月14日、2019年、ジュリアン君と父さん、成田。来てくれました、この時。その晩は、東京駅で何食べたっけ?親子丼か何かのお店に連れて行った覚えが。うちの子供たち。うちの妻の母はちょっと外出をして。何も食べない。こういうところで。これが最後のスタジオです。性格だから、俺の大学に来てもらいました。うちのメンバー、今もいるメンバーが少し。今もいるメンバーはさすがに何か。去年の修士2年生が、この時の3年生。お名前は、あれですよね。私とクリスト父親。一番左に出ている彼は、知っている人いるかな。ドイツ語の須藤先生。須藤先生は、須藤県の人がいるかもしれない。フランス語の木島先生の性格だから。県定室。彼は今、人工知能の専門家です。ZAPPという、SAPという、ドイツのマイクロソフトと言われる大きなソフトウェアカンパニーの研究者をやっているんですけど。こういう付き合いの友達が一人います。それだけですけど、これで終わりです。結構ね、彼みたいに長い付き合いの日本人、外国人を大事にしている友人がたくさんいるんだけど、さすがにここまで縁がある人ってすごくいないんですよ。だいたい日本であって、彼の本人となって、娘が突然日本に遊びに来るとかそういうことが起きて、今の非常に仲のいい。友人一家。今でも本当に、場所はいったり来たり、なかなか遠いんですよ。たびたび行けないんです。それで、写真がないんだけども、実は探せばどこかにあると思うんですよ。さっき写ってたこの一番右の、これがクリストの奥さん、アンネって奥さんなんだけど、奥さんのアンネは元々、彼女が元々ケルンの出身で、今は奥さんの実家の近くに彼らは住んでいるんですけど、それもあって、彼らのご両親、要するにこの子たちのおばあちゃんおじいちゃんが近くに住んでいるんです。そのおじいちゃんおばあちゃんの家にも連れて行ってもらったことがあって、おじいちゃんのお父さんはずっと絵を描くねん、仕事をしていた。家にいっぱい絵が飾って、もう数年前に亡くなったという連絡をもらいました。家にたくさん絵が描けてあったので、一枚ぜひ欲しいみたいな話をしたら、そのお父さん亡くなった後で、一枚あげるとわざわざ日本まで送ってくれたんです。奥さんの両親が住んでいる家の隣の家がかつてのメープピアニストのバックハウスという、ウェルダリム・バックハウス、あまり知っている人いないと思いますけれど、昔の戦後間もない頃のドイツのメープピアニストのバックハウスの家が隣にあるということで、実はそのご両親もあった。ピアニストとすごく信仰があって、それもあって私はバックハウスさんの子孫にも紹介をしてもらったことがあります。それでバックハウスが使っていた、本当に超有名なピアニストが使っていたピアノがその家に置いてあったんですけど、そのピアノを弾かせてもらいました。どこかにそれも動画撮られて、私じゃない人がユーチューブに変な日本人が来ているときのピアノを弾いていた人がユーチューブにあげて今もどこかで見られる。幸い私の名前とか書いてなかったことは探せないと思います。ということで、H&Mの話に戻ります。最初から聞いていた人はぜひ、小絵法を教えてください。学習の話を全体して、最後にこういう謎々を出しました。確率、みんなが知っている確率というのは、普通は全部足して1になるというのがいいと思いますけれど、サイコロにしても、コインを投げるにしても、ボールを取り出して色が見たいという問題であっても、実は基本的にこれまで知識が多くの確率というのは、微算的辞書に対する確率だったはずです。高校の数、いくつですか、確率統計の頃ってものすごいがっちりやった人は、実は高校でも連続の確率辞書を扱っているはずだけれども、多くの人は、高校の時に数学で確率統計でそんなに時間をかけてもらえなかったと思うので、その話を聞いていないのではないかと思います。今日はその話をします。雑談で時間を使っちゃった方も、それよりも私の方が通じやっていいと思います。どうして確率の話をするかというと、今から話があると思いますが、HFの中に出てきた出力確率というのを、どのスライドでもいいのですが、こんなのを使って説明をしたいはずです。この時の話では壺からボールが出てきて、それぞれの壺でどの色のボールも入っているのだけれども、出てくるボールの割合がちょっとずつ違いますよという説明をしました。この例でいうところの、壺に入っているボールのそれぞれの、例えば1番の壺から赤いのが出てくる確率、青いのが出てくる確率、それぞれ違います。それが出力確率です。この例外としては、これでもうパーフェクトですが、音声の場合、これはボールが感測しない、赤とか青とか緑というのが、音声の場合はスペクトルの形みたいなものに対応していますよという説明をしたと思うけれども、スペクトルの形って何種類ですかと言われても、無限にありますよ。しかも、2、3行じゃないと。連続量、今やってもらっている乱集しめん切りですから忘れずにやってください。このように、1つのフレームで囲ったデータファイルが、メルキャプスラの特徴を入れてありますけれども、不動小数点数が並んでいて、しかも1つのフレームあたり15個の数の組で表現されている。だから実際にはそういうパラメータですよね。かしこまれのDDの説明のときにスライドで見せたと思うけれども、これが今使ってもらっているデータファイルです。1行が1フレーム分析フレーム1つあたりにこういう数字がずらっと並んで1つの何かを表している。だからさっきのツボとボールのモデルと言ってみれば、このデータの1行を見て、こういう数の並びはじゃあ赤にしましょうと。こういう数の並びだったらここは青ですねみたいなことをやっているということなんだけれども、その場合に大体ボールの赤だろう青だろうといったらどういうふうにそれを分類すればいいのかという問題と、そもそも連続量だよね。連続的な数値を赤とか青とかという理算量に置き換えちゃっていいんです。場合によってはそれを置き換えずに確率を表現する方法もあるんじゃないのということが今日の主題です。今日の話は認識においては実はすごく大事なので、この動画の中で私がどういう説明をしているか、これも去年じゃなくて一昨年に作った動画だと思いますので、ちょっと流しながら後で補足をします。ちょっとこの動画が長いかもしれない。はい、動画がいくつかにわたってですね、HMMの隠れ丸コップモデルの話をしてきました。アルゴリズム、それから壺とボールのモデルというのを使った例題についてやってきましたけれども、ちょっとその壺とボールの話が続いてしまったために、一体どこが音声認識なんだというのが見えなくなってしまっているかもしれませんので、以前もこの話はしたのですが、改めてHMMにおける必要確率、それが音声認識、音声の特徴量とどうつながっているかを説明しておきたいと思います。2つの種類の確率、声位確率というのと出力確率というのがありましたが、そのうちの出力確率、Bという記号で表しましたけれども、それぞれの状態に対して定義されて、Bの入力は式でいうところのXT、これ実際に観測された何らかの値が入るのですが、音声の場合はここに時刻Tにおける音響特徴量ベクトル、以前やったようなパワースペクトルだとか、キャプストラ、この後で出てきますけれども、そういうものが入ります。観測値、あるいは観測シンボルといってもいいのですけれども、実際のところは音声シンボルではありませんので、シンボルに無理やりすることができると思いますが、壺とボールのモデルにおけるボール、これが音声ですよと言いましたが、ちょっとそこの表現には工夫が必要なのかもしれません。ボールの色、例題では赤と青というふうにしてしまいましたけれども、もちろん音声の種類を表現するのには、2種類、3種類では全く不足でして、ボールの色が仮に何百何千とあると、そう考えれば、いろんな種類の音声の特徴がボールの色として表現できるのかもしれない。その場合もボールというのはあくまでも微散的な特徴に過ぎませんから、もうちょっと一般的にはこのXTには観測値、特徴量ベクトルが入ると考えてください。それで今も言いましたが、よく用いられる特徴量ベクトルとしては、ケプストラムであるとか、メルケプストラム、MFCCという特徴量を以前に紹介したかもしれませんが、そういったものだとか、パワーだったり、そういうケプストラムやパワーの時間変化の特徴、デルタ特徴とか、デルタデルタ特徴といったりしますが、そういったものをうまく選択して入力として与える。したがいまして、通常XTに入る特徴量は数十次元程度のベクトル量、連続値で表現されたベクトル量になるわけですね。ツボとボールのモデルのボール、赤いボールといっていたものが実は数十次元からなる連続量の特徴量ベクトルに対応している。そういうふうに捉えてください。そういうわけで、確率の計算、Bという出力確率を計算する際に、与えられる入力が離散シンボルであったり、連続的な特徴量、しかもベクトルであるということから、確率を計算する際にもいろんなやり方が考えられる。どれが正しい間違っているのではなくて、さまざまな留意があります。この後で少しその話をしたいと思います。ここで確率についてある程度皆さんは知識があると思いますけれども、念のためにちょっと、離散的な確率と連続的な確率についておさらいをしておきたいと思います。おそらく皆さんがこれまでに数学の授業とかで取り扱ってきた確率のかなり多くの部分は、離散的事象に対する確率だったんじゃないかと思います。多くの部分というか、要するに小学校、中学校の頃から触れてきた確率というのは、多分離散的事象に対する確率。ですから確率分布という意味では、これは後で出てきますが、離散確率分布に対応する確率ということになりますが、例えばコインを投げるとか、サイコロ振るとか、この間の例題でやった壺からボールを出すという問題も、事象としては離散的である。もうちょっと数学的に言うと、起こり得る事象の数が有限個、有限種類ですね。サイコロ振るという場合も典型的な場合は、6つの面があって、その6つのいずれかが出る。それでもちろんその際に全ての面が等確率かどうかというのは別に違ってもいいと思いますけれども、確率論的には、起こり得る事象の全ての確率を足し算すると必ず1になりますよ。確率が1ということは、要するに起こり得る全ての現象がそこに含まれているということですよね。ですから離散的事象の場合は、たとえどんなに複雑な形をしていても、あるいはコインとかサイコロが仮にいびつな形をしていたとしても、起こり得る事象の全てを足すと確率が1になると、そういうふうに定義がされます。それでは連続的事象に対する確率はどうなのか。もちろん連続的な事象というのがたくさんありますので、それに対しても確率をうまい前に定義しなければいけないのですが、こういうふうに考えてみてください。例えばサイコロというのは、普通は正六面体というんですかね、一方体と言ってもいいんですけれども、全ての面が正方形の一方体を使うことが多いですけれども、正南面体というのは、正六面体よりも少し大きな数でも作ることができて、例えば正二重面体などがありますよね。そういうサイコロを作ったとすると、もし全ての面が等確率で出るならば、ある面が出る確率は1二分ということになります。もし正南とか面体にこだわらなくてもよければ、すなわち面の形が全部違ってもよければ、いくらでも目を増やしていける。その際にどんどんどんどん面の数を増やしていって、もしも無限の面があるようなサイコロを作った場合に、そのサイコロってどういう形になるだろうか。等確率ということを考えるならば、それはおそらく球ですよね。ボールみたいな形になる。仮にボールをサイコロと見立てて、それを投げる。コロコロ転がって最後どこかで止まりますね。止まったときに一番上に来ている面、これも面というか限りなく面積が0に小さな面、点としてしまうと解析的におかしくなりますが、限りなく面積が小さい面が上にあると考えるならば、その面が出る確率はどうなるかというと、無限の面があるということで、6面のサイコロと同じ流儀だとすると、無限大分の1ですね。無限大分の1は皆さん知っているように、限りなく0に小さな数であるはずですが、一つ大事なのは、その無限大分の1の確率が全部合わさると、つまり起こり得るあらゆる確率を全て足し算すると、1にならなきゃいけない。確率というのはそういうふうに定義されるものですから、連続的事象であっても、起こり得る事象の数が無限個、かつ全ての確率を足し算して1にしなければいけない。ここで微分積分と同じで、解析的に無限大に近づく、あるいは確率が0に近づくという考え方で、事象に対しては有限個ではありませんので、足し算して1というのは数学的には難しいということで、積分値が1というふうに考えましょう。そういうことで、いろんな連続的な確率の表現があるのですが、あとでやりますけれど、例えば分布が正規分布の場合は、全ての定義、マイナス無限大から無限大まで定義が決まりますので、その間の積分値が1になる、そういうふうに定義されます。音声のスペクトルパターンとか、ケポジトラムとか、そういうものももちろん連続的事象、もともとは連続的事象なので、この考え方を当てはめる必要があるのです。赤字で、離散確率分布に対応付けて言葉が入れてあります。これはちょっと言葉、余分な単語が入っているので、何か違うように思うかもしれませんが、連続事象のときは確率分布といっても、離散のときと同じようには考えられないので、確率密度という言葉を使うということで、厳密には連続事象を表現する確率分布は、連続確率密度分布といったりします。したがって確率と確率密度というのは、そもそも定義の仕方が異なりますから、これを気をつけたいのは、ごちゃ混ぜにはできません。確率と確率密度をそのまま足し算すると、おかしなことになります。もちろん適当な重みをつけるとか、完全に独立とみなして別々のパラメータとして扱う分には、混ぜることも不可能ではないのですが、離散的な確率と連続的な確率というのは、そもそも元々の定義が違うということは気をつけていてください。説明されたとおりですが、連続量って結構、今我々が扱うセンサーから取り込むようなデータというのは結構連続量ですよ。表か裏かみたいな、そういうのは例題としてはもちろんわかりやすいんだけれども、画像にしろ音声にしろその他のセンシングデータ、大体連続量です。何に使うか色々ですが、識別問題を解くという場合も、入力されたデータが音声の場合はあなのか否のかを識別したい。これもですね、もう一回以前のスライドを映します。こういう話をしたいと思いますが、この中で音声認識というのは音声を分析して、何らかのパラメータXみたいなのが出てきたときに、このXがそもそもこの話者がしゃべろうとした何とかという単語にどういう関係になっているのかがちゃんとうまくつながればそれが認識ができたということなんですが、確率で表現するとこの一番下に式があるように、この条件確率でXという事象が観測されたとき、つまりXが条件になったときにそれがWである確率。Wである確率というのは、IUAを読むとしたら、今観測された音声波形があの確率がいくつですか、何パーセントです、否の確率が何パーセントです、IUAの全部について確率値を出して、認識問題というのはその一番高い確率を与えるものが正解ですよという風にやればいいのですが、これ難しいのはこの条件に入る側で、左側のWはこれは人間が定義すればいい話だから、音声にしろ単語にしろ無限ってことはない。もちろん数がすごく多くなるかもしれないけど、単語の数も一応この認識システムは1万単語を扱うことができますように決めてやれば、それでも有限このWに落とし込めます。だけどXの方はどう考えると無限ありますよ。だからいくらでも作れるし、しかも連続量で。だから実はこれ具合が悪くて、このまま計算したくても、あらかじめ全てのXについて調べておきますということができない。それでこの時の説明ではしょうがないからこれひっくり返しましょうと。XとWひっくり返すと後ろにちょっと呼ぶのがついちゃうけれども、これならば計算ができるよ。だけどこの時に計算ができるXの確率というのが、今度は条件じゃなくなるからその確率を与えるそのものです。こっちが無限パターンあるということは、連続量に対する確率を何とかして計算しなくちゃいけない。どうすればいいんですかというのがさっきの話です。今の話は実はここをどうやろうか。今まではこれは簡単のためにXが赤と青と緑で例えましょうとやっちゃったから非常にシンプルにできたんだけれども、実際にはそうはやらない。じゃあ続きが半分くらいあります。それで順序的にどちらを先に扱うのが適切か、実は難しいんですけれども、慣れているということで、離散確率分布で音声認識をするということを考えたい。ツボとボールのモデルのボールも離散的な現象ですから、ここではXT入力となる音声の特徴量をわざわざシンボルとして表現すればいいんじゃないですかということですね。シンボルというのは要するにツボとボールのボールの色に対応するような何か、ラベルを音声の特徴量に振ってあげればいい。そういうことで色が何種類あるかはともかくとして、あるシンボル、赤とか青とかそういうボールのシンボル表現が、それぞれ特徴量ゲットの性質に対応していると考えてください。実際にはいろいろやり方があるんですけれども、離散確率分布でパターン認識をやるときに最も典型的に使われる方法を一つ紹介したいと思います。少々してこの後で出てくるベクトル量子化という言葉を使うんですが、手順としてはまずベクトル量子化のための前準備としてクラスタリングという処理が行われます。最近はクラスターという言葉は感染者の集団を表すらしいですけれども、クラスターというのは単に集合とか集団という意味しかありませんので、感染とかそういう意味は含んでいないですが、世の中の人はクラスターと言うとちょっと警戒されてしまうかもしれませんけれど、統計的には、あるいは統計学的には昔からクラスタリング、あるいはクラスター分析という言葉もありますけれども、集合、あるいは集団に対して処理をする、あるいはたくさんのデータを集合にまとめるという意味でクラスタリングという言葉が使われます。具体的な手順が書いてありますが、まずたくさんのデータが必要である。音声の場合はたくさんの特徴量、ベクトル、ケープスといったものをたくさん集める。一つや二つではあまり意味をなしませんが、たくさんのデータを集めます。そのデータに対して、ここちょっと大事なのですが、データの分布に基づいて、スカラ量だと割とヒストグラム作るの簡単なんですが、ベクトル量の場合は全ての空間を対象にしようとすると、おそらくほとんどの場所にはデータが一つもないということになりますので、あらかじめデータ分布を見た上で適切な設定数の分布区間に分けるということが行われます。いろいろと有名なアルゴリズムがあるのですが、例えばここに書きましたけれども、経平均法がよく知られているアルゴリズムですけれども、部分空間に分けた上で、それぞれにシンボルを付与する。ここでもシンボルは別にボールの色も赤とか青でもいいですけれども、実際にはコードというのですかね、1番、2番、3番というふうな番号を振っていくことが多いですね。コンピューターでは使いやすいということで。ただそのときの、仮に番号を振ったとしても、1番、2番、3番の数字には代償関係のような意味はありません。単に背番号のような意味しか持っていないので、2番のコードが1番の何かの量が2倍であるということはありません。ですからシンボルというのは数字でもいいし、アルファベットでもいいと思いますし、赤青黄色みたいな色でもいいと思いますけれども、そういうコードに持ち替える。したがって各部分空間、1つの部分空間が何番というコードで代表的に表現されることになりますが、実際にはその部分空間には例がたくさんあるわけですから、クラスタリングの結果として保持するべき情報としては、その部分空間を代表する点、これをセントロイドといったりしますけれども、代表点の座標、複数次元、場合によっては何十次元もの空間にデータが散らばっているわけですので、その代表点の座標を決めて、コードブックと呼ばれるデータベースのようなものに格納しておく。陽性認識の場合も、こういう手順でやろうとすると、クラスタリングの結果、コードブックには、そうですね、だいたい何百何千という部分空間の代表点が集まることになりますから、その一つ一つが特徴量別というのも次元数を持っているということで、相当複雑な規模の大きなコードブックが出来上がります。このように準備をしておいた上で、したがってクラスタリングというのは、ある意味学習のプロセスと言ってもいいのかもしれませんが、実際のデータに対しては、手順2としてベクトル量子化、VQと言われたりします、Vector Quantificationの略ですね。量子化という言葉は、いわゆるコンパクトディスクに音を格納するときの量子化と同じでして、連続的な量を理算的な表現に置き換えることですけれども、それが音のサンプリングとは違ってベクトルになっている。しかもその入力されるベクトルが場合によっては何十次元、何百次元というベクトルですので、それを量子化するということになりますが、先ほど作ったコードブックを使ってですね、入力されたベクトルに対しては、最も近いセントロイドを探しましょう。学習のときに大量のデータを使ってですね、部分空間を最適に分割してあるので、一番近いセントロイドを探す。そうやってして、そのセントロイドが対応しているコード、シンボルですね、番号だったりアルファベットだったり、元ボルドモデルの場合は赤と青とかそういう色になりますけれども、これを割り当てればいい。手順としてはですね、このVQの手順はそれほど時間はかかりません。なぜかというと、すでにコードブックを持っているわけなので、一番近いのを探すプロセスというのは非常に簡単です。もちろんそれに対して準備の段階のクラスタリングはそれなりに時間がかかるプロセスだと思います。はい。ここで言おうとしていることが、今の説明でどんなに通じたかわからないけれども、こういうふうにイメージをするとわかりやすいかなと思います。ベクトルじゃなければ、ベクトルじゃなくても簡単ではないかもしれないけれども、例えば英語と数学の点数みたいな例を使うのがよくありがちなんですけれども、もうちょっとインパクトがあるんですね。例えば人間の身長と体重みたいなものを考えると、大勢の人がいます。10人、20人だと意味がないけれど、1万人の人がいるとして、その人全員の身長のデータがあるとします。そうすると、多分そのデータの分布というのは正規分布ですよね。平均的な身長の人が一番人数多くて、むちゃくちゃ背が高い人とかむちゃくちゃ背が低い人は少ない。そういう分布だよね。その人たちの集団、1万人の集団を身長によるグループ分けをしましょうと。例えば1万人を5グループに分けてください。身長の違いによって5グループに分けてくださいとやると、これ、もしも正規分布でなくて、まんべんなく分布しているならば、5分割すればいいわけでしょ、横軸を。だけど正規分布みたいな場合は、X軸横軸に対して5分割するんだと、真ん中のところにものすごい人が増えて、両端は少なくなります。だからもし人数を揃えたければ、平均付近はうんと横軸の幅を狭める必要があるし、そこから離れるところは広くしないと人がたくさん入らないんですね。スカラリオの場合も、きれいに分割きれいというのは、だいたい等分割するためにはそういう分布をしないといけないというのは、たぶんこれ理解できるでしょう。問題はそれが複数次元、ベクトルになったときにどうなるか。横軸が身長、縦軸が体重について、そういう分布、その底面に1万人の人のデータを取るとするとします。つまり身長が高くて体重が重い人というのは、たぶん右上の方に行くし、逆の日から左下の方に。もちろん身長が高いけど体重が軽い私みたいな人は、右下の方に行くかもしれん。だけど平均的にそういう風にプロットすると、おそらく右上がりの対角線上にデータが結構集中しますよね、イメージ的に。まんべんなくその平面に分布することはありえない。たまに外れきしの人もいるかもしれないけど、身長が低いけど体重が重い人みたいな、そういう人いるかもしれないけど、数としてはそんなに多くないから。その平面をやっぱり大体人数が同じになるように分割してくださいと言われたらどうしますか。そうするとやっぱりさっきと同じ考え方で、真ん中付近、平均値付近の面積は小さくしてあげないと、全部同じ面積にしたらそこの平均付近にたくさんの人が入っちゃうから、平均付近は面積小さくして、そこから外れるほど面積をちょっと大きめにしないと人数揃わないんです。2次元でも結構めんどくさい感じがしますよね、それをやるために、ちゃんと合理的にやるために。それが、それこそ何十次元になったらどうするんですかという問題です、これは。だから一見そんなの等分割すればいいんじゃないと思うんだけれど、等分割しちゃうと、おそらくデータが一個もないような空間が山ほどできてしまって意味がなくなると。逆にデータがすごい多い空間がいくつかできてしまうというのも、識別問題を解くためにはあまり嬉しくないです。そのためにこういうクラスタリングなどの処理をしなければいけない。この説明をもうちょっと続けて。はい、それで今の理算的な処理、クラスタリングをしてからベクトル量子化をするというやり方で、線認識をする場合はどうすればいいかというとですね、書いてありますけれども、あらかじめたくさんのデータについて、VQの結果であるシンボル、背番号のようなものと言いましたけれども、特徴量Xの代わりにVQシンボルCと書きましたが、このCと言語カテゴリー、音素とか単語とかとの関係をあらかじめ調べておけばですね、以前に出てきた条件付き確率、確かあの時はPのW×Xという表現だと思いますけれども、このXがCに置き換わることで、このCに対するWの条件付き自分確率が直接計算できます。以前の説明ではこの確率を直接計算できないから、ベースの定義を使って音響確率と言語確率に分解するんですよと説明しましたが、実はベクトル量子化を使うとですね、この確率を直接計算することができます。イメージとしては下に簡単な絵が書いてありますけれども、これは二次元平面に対する説明なのであまりいいところないかもしれませんが、このようにしてですね、平面をいくつかの部分空間に分ける。どうしてこんな鉢の隅台にしたかというと、これは機械的に分割するならば単純な四角面図を作ればいいんですが、実際のデータの分布に対して最適な分割の仕方、最適というのはデータの数が統計的に偏らないようにということで、例えばデータが密集しているところは部分空間の大きさを狭くする、小さくするとかね、ほとんどデータがない場所は全部まとめてしまうということをする、それが必要になるんですけれども、絵の中で×で表されているところがセントロイドの座標です。ある空間の大臓的な座標、重心みたいなものだと思っていいと思いますが、それに対して例えば赤い丸のようなデータが入ってくると、この一番近いセントロイドということで今編成でつながれている×が選ばれる。したがってこの赤い丸はその同じ部分空間に属するシンボルが割り当てられるというふうに考えてください。このやり方のメリットは、実はあらかじめPのW、縦棒Cというのは両方二三次的な量ですよね。言語カテゴリーもそれからVQのシンボルも有限個ですので、あらかじめ全て関係を調べておけば、実際に認識するときに計算しなくても、その計算された表を見るだけで数字が使える、そういうメリットがあります。そのことをテーブルルックアップと言ったりしますが、あらかじめ確率を計算して表に収めておく。実際にリアルタイムの処理のときにはそのテーブルを見るだけで確率がそこに入っていますから、計算量非常に精査できますよね。その反面、デメリットが何かというと、当然今絵に書いてあるように実際のデータは赤い丸なのに、それが全てこの×のところをセントロイドで代表されてしまう、置き換えられてしまいますから、この点線のところが誤差になるわけです。それが欠点です。ちょっと補足をしておきますと、この理算確率分布に基づく音声認識、ベクトル量子化を使った音声認識は、今から30年くらい前ですかね、コンピューターの能力がまだ十分じゃなかった頃には結構よく使われていましたけれど、さすがに最近はあまり見ません。時々非常にコンパクトな音声認識でいいよという場合に、たまに使っているケースを見るときがありますが、ほとんど今はこういう技術はもはや必要なくなったといってもいいのかもしれません。私が学生自体はかなりこういうテクニックをよく使った記憶があります。ですから考え方としては知っておいてそうはしませんけれども、今こういうやり方を実践の場で使うことはほとんどなくなったと思っていてください。休憩を入る前に、さっき出てきたクラスタリングの考え方で、先ほど身長と体重の例を出したけれども、もうちょっと大学の授業でたまに話すことがあるトピックで、1時間、2時間くらいかけて調べるスライドの一番最初の例題だけちょっと皆さんにお見せします。ちょっとあまりわかりやすいスライドじゃない?これに1枚見せられたところでという感じですが、こういう例題。これを見てください。こういう問題、これ非常によく使われるクラスタリングの考え方を説明するのに非常に便利な例だったんですが、書いてある通りなんですけれども、ある街に多数の家が建っているとします。家が建っていると言っても、普通家、本当に雑然と家が建っているような街を想像してください。例えば、遠感覚に綺麗に並んでいるような家ならば問題かもしれない。普通街の中の家は道だってそんなにまっすぐじゃないだろうし、ところどころ空き地があったり、小さな家があったり大きな家があったりということがあると思うので、とにかく何百個って家がある、そういう街を想像してください。その街にN個の郵便ポストを設置したいとします。郵便ポストでも電話ボックスもなるんですが、ここで郵便ポストを設置したい。郵便ポスト、多分ですね、郵便ポストみたいなものは今は郵便局を運営化されたけれども、需要としては、郵便ポストってそこら中にあるけどね、当然人がたくさんいるところにはたくさん置きたいという思いがあるだろうから、ポストをどこに置きましょうか、みたいなことを考える場合は、いろいろ最適な場所を見たいなと考えると思います。だから、郵便ポストを設置する条件として、一つの条件は、各戸、つまりある家に住んでいる人は最も近いところに設置されたポストを使います。それが条件その1です。逆にポストの側から見ると、それぞれのポストはそのポストを使う人、使う人というのは当然、家から見ると一番近いポストを使う人が決まるよね、街の中でね。何百戸の中で例えば50戸くらいの家はこのポストを使いますという風に決まったら、そのポストを使う50件の家のすべての家から最も近い地点、つまり平均的にということですけれども、ポストをどこに置くかという問題は、使う50件の家の中の端っこに置いたんだと遠くなる人が出ちゃうから、真ん中らへん、化学的に言うと重心付近にポストが置かれると具合がいいですよね。この条件を満たすようにポストの場所を決めてください。Nは最初に与えていいと思ってください。5個のポストを設置してください。1個のポストの場合は問題は多少簡単かもしれない。1個のポストを設置するだけならば、すべての家の座標的、気化学的ないわゆる重心付近にポストを置けば、そのポスト全員は使われるだから、一般、そのポストから各家までの距離の場合、一般、小さくなる点というのは各重心付近ですよね。1個ならば問題は簡単なんだけど、2個以上のポストを置く場合、5個のポストを置いてくださいとするときに、これどうやって解けばいいんですか。問題は1発で決められるのかという問題です。なぜならば、まず適当にポストを置いてみましょうとやるでしょう。5つのポストを適当に置きます。5つのポストを適当に置いた瞬間に、それぞれの家庭の人はどのポストを使うかが決まるよね。ポストの人が決まるから。だから一番近いポストが見つけられる。そうすると逆にポストの側から見ると、そのポストを使う人がそこで決まりますよね。だけどそのときに2の条件を満たしているか、ちょっとこれ怪しいよね。適当に読むと。2番目の条件というのはそのポストを使う全員から一番近い地点、平均が一番短くなるような平均とかは合計でいい。ポストからそのポストを使う家までの距離の合計が一番小さくなる場所に置いてくださいという条件が2番なので、その今最初に置いた場所がその条件満たしていないとすると、じゃあちょっとこっちにずらそうということをやる必要がある。ところがちょっとずらしたと、1番の条件が狂いますよ。つまり最初に置かれたポストが別の場所に移動することによって、それまでAのポストを使っていた家がもしかしたらBのポストに近くなるかもしれない。あっこっちにポストがあるうちはこっちに使おう。そうすると今度また2番の条件が狂いますよ。という具合にこの問題は実は一発では解けない例題として調べています。この問題を解くためのアルゴリズムというのが、実はさっきちょっと出てきた、経平均法、平民のアルゴリズム、かなり有名な方法があるんですが、ここでその説明をすると、スライドが30枚あるのでやめておきますけれども、考え方としては、ざっと見せましょうか。ちょっと後ろに、イメージで掴めればいいと思うんですけれども、例えばということは、式とかを無視してください。下の枠の中の絵だと見て、この丸が家だとします。赤い四角がポストだとします。最初は適当にこうやって赤いポストを置いてあげると、その瞬間に、これ実際に適当ですけれども、それぞれのポストを使う家が決まる。道の条件とか何かも含めると、こうやってして、このポストはこの家の人が使います、このポストはこの家の人が使います。ところがそうするとさっきの話で、一番、例えばこのポストの場合はどう考えても、この丸の人たちが使うといっても、場所が良くない。このポストはもっとこの辺にあるべきです。ということで、こっちに動かす。重心付近に動かす。そうすると今度、最初に引っ張った線がずれます。わかります?このポストの場所を動かすことによって、一番近いポストを使うという条件がちょっとずれますよ。そうするとまた、そのポストを使う人の集合が変わるから、またちょっとこれ動かないようにしてください。ということを延々と繰り返す必要があります。延々と繰り返すんじゃなくて、徐々にこれは収束に向かいますよ。どこかで止まるだろう。止まったらそれが、その点が一番最適な点ですみたいな、そういうアルゴリズムがあるよというので、実はちょっと最近あまり時間があってやってないんですけれど、私の大学の授業でしているというこの一つです。ちなみに私の卒業論文のテーマでもあったんですけれども、定型分割を使う。それが面白いです。こないだ、HMの学習のところで、EMのアルゴリズムによると鉛筆を並べるみたいな話をしましたけれども、鉛筆の消化系が一発で決められない問題に、しかも複数次元になるというのを紹介をしました。ちょっと2、3分休憩しましょう。3分休憩です。3分休憩です。3分休憩です。3分休憩です。では、実は非常に簡単でして、XPそのまま連続的な入力子として使えますから、そういった数十次元の特徴量がそのまま確率の変数になるわけですね。ただ、そのときにどんな関数を使うかというところで、いろいろな考え方が当然出てきます。典型的なやり方は、正規分布を使おうと。正規分布はご存知だと思いますが、ノーマルディストリビューションといったり、ガルシアンディストリビューションといったより一般的というのかな、研究者がよく使う表現ですけれども、ガウス分布というものですね。もうちょっと具体的には平均値と分散値で定義されるよくある分布です。我々の身の回りのいろんな現象は正規分布に当てはまることが多くてですね、例えば皆さんにテストをかかるかをやったときの特典とか、水が少ないとそんなにきれいにならないんですけれども、普通の人に試験、しかもちゃんと作られたテストをやるときれいな正規分布になることが調べています。この話は水曜日の方を撮ってくれている人は最初の方で話をしたと、もっとちゃんと正規分布を話したと、ただ数値の2をとっていないというかもしれないという話を。あとですね、例えば我々の人間のいろんなサイズ、身長とか体重とかね、これも多くの人を調べると、平均値付近の人が一番多い。そういうことを我々は何となく案に分かっているとも言えますけれどね、実は音声についても正規分布が当てはまる特徴が多くてですね、もうちょっと具体的に言うと、たくさんの人の声を集めてきて、アワースペクトルなりテクストラムなりを計算すると、平均的な値というのがベクトルで出てくるわけですけれども、平均の声というのがどんな声かというと聞いてみるとちょっと面白いんですけれども、平均値付近の音声の特徴を持っている人が多くて、そこから離れるほど人は少なくなる。直感にも合っていると思います。滅多に聞かないような声を出す人の数は少ないという、そういうことですから、正規分布が当てはまるのが一番典型的と言ってもいいと思います。ちょっと横に話をばかりすると時間がなくなっちゃうんですけれど、平均的な声、これ簡単に作れるんですけれども、聞いてみると何ともつまらない声ですね。やっぱり声ってすごく個性を含んでいるから、これまたいい声とか悪い声というのも、何がどうなればいい声かって難しいけど、でもあるよね。歌とか聞いて、この人声がいいなみたいなことってありますよ。だけど真似しようとしてもなかなか難しいです。平均的な声というのは実はすごくつまらないことです。同様に、これも有名な話かもしれないけど、人の顔のデータをたくさん詰めてきて、平均的な顔を作る。目の位置とか鼻の位置とか髪の色とか、形とかそういうのを全部調べた上で、平均的な顔を作るとそれはどういう顔かと言って、やっぱりちょっと語弊悪くもしないけどつまらない顔です。東大のある顔の研究で有名な先生があるとき、これは半分雑談だったときに、顔のデータをたくさん詰めて、いわゆる、これもどういう表現だったか忘れましたけど、かっこいい人をたくさん詰めるとか、かわいい人をたくさん詰めるみたいなことをやるわけです。美人の顔をたくさん詰めてきて、美人を100人集めて平均値を作れば美人ができるようになると、すごいものってやっぱりつまらない顔になるんです。だから、正義文化になるとは言っても、やっぱり人の特徴というか、我々がその人の顔を見たり声を聞いたりしたときに、何を捉えているかというと、おそらくみんな同じようなところってあまり気にしないんだろうね。人の顔をパッと見たときに、誰でも持っているような、誰でも同じような特徴には目が行かない。だけど、他の人とちょっと違うなというところはすごく気になるんだと。目のパーツの形にしても声もそうだよ。だからおそらくデータとして調べると正義文化になるけれども、人は正義文化的には見たり聞いたりしていないと見れる。その辺は非常に面白いです。やってみると。正義文化のような。それでこの辺はどうですかね。皆さん請求文句はもうどこかでちゃんとエンドとしているんじゃないかと思いますけど、あまりこの授業ではしっかり説明しないつもりなんですけれども、ざっとおさらいをしておくと、一次元の請求文句はこんな式でしたよね。参考書によって、エクスポネンシャルexpというのが、これはeの何とか上ですから、eの右側にこの大格好が小さく載っている表記の参考書もあると思いますけれども、ちょっと小さくならないためにexpを使っていますけれども、この式、よく初めて見る人にはびょっとされることがあるんですが、実は変数は右の大格好の中に入っているxという、これだけが変数で、これ全体がxの関数になっていますので、その他の記号、πはもちろんですし、x÷これが平均値ですね、それから分母に2カ所ある、シグマの事情というのは分散値ですね。これらは正規分布を定義するときには、もうすでに変数として与えられていますから、実はxに関する単純な関数と言っていいと思います。右上に、ちょっとこれは多次元で書いてありますけれども、このように格好した分布が正規分布の典型的な形です。それでこれが多次元になるとどうなるかというと、こういうふうに式が少し複雑になりますけれども、変わったところは、ベクトルになるということで掛け算の順序がちょっと関係してきますから、右の方の格好の中、x-x÷というところがxのベクトル、これが入力の多次元の特徴に対して、x÷もやはりこれもベクトルですね。それからシグマ、この式の中にある大文字のシグマは、いわゆる足し算の記号のシグマではなくて、共分散行列と言われるものですけれども、共分散行列が2ヶ所に入っている。あとでちょっとこの共分散行列については触れますが、平均ベクトルと共分散行列を使って、こんなふうに多次元の正規分布を表現することもできます。詳細は省略しますけれども、こうやってして求められた確率を使ってHMMを動かすことをすれば、それが連続モデル、連続分布のモデルになるということです。それで連続確率密度分布を表すのに正規分布を使う場合ですが、もちろん数学的には正規分布というのがそんなに何種類もあるわけではないので、先ほどの式でそのまま計算すればいいのですけれども、特に多次元になった場合、入力する特許料の次元数が何十次元になった場合は、この正規分布の式を真面目に計算すると、ちょっと問題が発生するときがあります。真面目にという意味で定義式通りの計算のことを、わざわざ全共分散正規分布と呼ぶような人がいますけれども、英語ではフルコパリアンスが次元と言ったりするのですが、これは全とかフルという言葉がついていても、数学的に正しく計算するという意味にすぎません。このフルコパリアンスという言葉が持っている意味が、共分散両列というのが過去に勉強したことがある人はわかっていると思いますが、入力するベクトルの次数、もしも十次元のベクトルが入ってきた場合は、ベクトルの次数が10ですけれども、共分散両列はそいつの正方行列ということで、10×10の100個の要素を持っている行列になる。さらに次数が増えた場合、入力データが100次元だとすると、要素が1万になりますよ。もちろんその1万になろうが、ちゃんと計算できれば全く問題ないのですけれども、単純に考えて1万個の要素を持っている共分散両列のパラメータを計算するのに、入力データが1000個しかないという場合も起こり得ます。データの数というのが当然集められるだけしか使えませんので。定義式というのは数学において、無限のデータが与えられるという条件が暗黙に存在する。ですから実際にはデータの数が有限か、あるいは少なかった場合には、共分散両列のパラメータがちゃんと計算できないことって結構あるんです。そうした場合の工夫としていくつかバリエーションが存在します。ちょっとそれを紹介してみたいと思います。まずですね、その全共分散、定義式通りに計算する共分散に対して、対角成分だけを見ましょう。本来は対角成分以外のところに、いわゆる次元間の相関の情報が入ってますから、覗いてしまうのがだいぶ乱暴なんですが、計算量の節約という意味では対角成分だけを使うというのは、一応多少意味があるのかなと思いますね。これものすごくざっくり言うと、分散についても共分散両列ではなくて、分散ベットルにしてしまいましょう、そういう発想だと思えばいいわけです。したがって、パラメータの数はかなり減る。先ほど、例えば100×100の場合は1万と言いましたが、対角成分だけで良ければ、100次元の場合も共分散両列は100で済みますよね。ですからパラメータの数はかなり減りますが、当然その分表現能力が落ちてしまう。今ちょっと言いましたけれども、次元間の相関の情報が一切見られなくなってしまいますので、このバランスを考えて表現能力が多少落ちてもパラメータ数を減らしたいという場合によく使われる方法です。今の対角共分散というのは、パラメータの計算量を減らすのには最も手っ取り早い方法ですが、さすがに表現能力が落ちすぎてしまうということで、接注案というんですかね、全共分散と対角共分散のいいとこ取りをするような考え方として、最もよく使われているのがこの混合正規分布というものです。英語ではミクスチュアガウスティアンといったりしますね。ガウス分布をミックスするという意味ですけれども、見かけ上、全共分散正規分布の表現能力を保ったまま、対角共分散とほぼ等々、若干プラスアルファなんですけれども、パラメータ数で表現ができるということになります。考え方としては式に書いてありますけれども、確率密度関数FXをいくつかの正規分布の和で表現しよう。一つ一つの正規分布は、例えば対角共分散のシンプルなものであっても、何らかの重み係数でそれらを足し合わせることで、結構複雑な形も表現できますよ、そういうアイデアです。それからもう一つですね、これも長年よく使われてきたアイデアでして、研究者によっていろんな表現を使うんですが、私がこういうことに関わっていた頃は、半連続とかいう人が多かったんですけれども、後でちょっとこれ意味がわかりにくいということで、あまり使われなくなりましたけども、英語の表現である対同ミクスチャーという言い方がよく知られているかもしれません。対同というのは、結びつけるとかつなぎ合わせるとかそういう意味で、ミクスチャー、先ほどの混合正規分布をいくつか結びつけるという、そういう意味を持っているんですけれども、これ具体的にはですね、先ほどお話しした離散分布、ベクトル凝縮化を使うやり方のセントロイドの表現に連続確率密度分布を与えておくと、単にそこにシンボルがあるとするのではなくて、それぞれのセントロイドに正規分布を割り当てておくことで、見かけ上はその正規分布の足し合わせで表現されますよということで、既定分布を共有するという言い方を使うこともありますけれども、そういうふうなアイディアが提案されています。後で時間があったらちょっと詳しく説明したいと思います。はい、それでちょっと言葉だけだと直感的に理解しにくいかもしれませんので、これかなり昔のある解説記事から引用した図なんですが、いろいろな確率分布の表現がどのくらいの能力を持っているのかを直感的に理解しやすいかなと思ったので、先ほどの説明に加えて少し紹介してみたいと思います。今回は多次元ということで何十次元という話をずっとしてきましたが、何十次元の空間をスライド上に平面上に描くことはできませんので、この絵は二次元で説明してあります。XY平面上、二次元の平面上にデータがブープしていたときに、いろいろな確率分布が何をどういうふうに表現するかという概念図のようなものですけれども、まず一番左上のAと書いてあるところでサンプルディストリビューション、点がいっぱい散らばっていますが、これが実際のデータ分布だとします。空まめみたいな形をしてちょっと右上がりの特徴を持っている分布だと思えばいいなと思います。この点線で囲んだ空まめみたいな形を何とかして上手に表現できればいいのですが、最初のほうでやりました離散確率分布を使った考え方が、その右にあるBとC、Cについてはこの授業中は説明していませんので、Bを見てほしいのですが、シングルコードブックと、このデータの分布になるべく忠実に離散分布をクラスタリングによって学習すると、おそらくこれはざと歪ませてあるのは、データの分布をそこまで忠実には学習できないということを表しているようですが、データの散らばりを極力忠実にモデル化できる。当然この場合はXY平面上とはいえ、離散分布というのは特に関数で推理的に表すということではないですから、こんなふうな歪な形になるだろうと、そういうふうに理解すればいいのです。これも一つの考え方としては、元のデータ分布に近づけるという意味では非常に有用といってもいいのかもしれません。それに対して下の3つ、D、E、Fというのが正規分布を使った表現なんですが、まず真ん中を見てください。D-covarianceというもので一つの正規分布で表すと、このケースではXとYに多少の相関がある。正規分布の表現の仕方は、2次元平面上で楕円が書いてあって、この楕円が何を意味するかということですが、わかりやすく考えるならば、平均から離れている距離が近い分散値を一定にしたところをつなぐ。何でもいいですが、標準偏差のところの等しくなるところを結ぶと楕円ができるのですが、XY平面上で本来正規分布というのは、相関を表す成分がこの楕円の傾きに相当しますから、例えばこんなふうに当てはめることができるだろうと。元々の分布が点線で書いてありますが、何となく重なるような形が関数、パラメトリックな表現といったりもしますが、正規分布の式で当てはめることができるわけです。ところが先ほど言った理由で、今は2次元ですので共分散行率はそんなに大きなサイズにはならないのですが、考え方として、ダイアゴナルコバリアンス、対角成分だけにするというのがどういうことかというと、左側にDと書いているこの絵がそれに相当しますが、対角成分しか使わないと相関の情報はどうやっても表現できないので、それはこの絵でいうところの楕円の軸がX軸、Y軸に平行にしかなり得ないということで、どんなに頑張ってもこのぐらいの表現能力しか持たないわけです。最後にF、ミクスチャー伝説と書かれていますが、先ほど説明したミクスチャーガウスイアンに相当する考え方です。この絵では今4つの楕円になっていますけれども、この4つの楕円それぞれは軸がX軸、Y軸に平行である、すなわちダイアグナルの正規分布と、相関を見ないような正規分布で表されているとして、それを組み合わせることでこのように点線のグループをきれいに表現できる、足し算で表現するということはこういうことですけれども、複数の正規分布で一つの大きな点線で囲まれたグループを表現してあげるということができます。この絵では4つになっていますけれども、実際には混合する数はもっと多くても構わないので、何十という正規分布を足し合わせてミクスチャー表現するということがよく使われているようです。この絵、簡単に伝わりましたかね。特に連続グループについては、この対角共分散、それから真面目に全部計算するののしがいというのは、あくまでもこれは概念図なので、この台数の中で、正規分布のある一定の確率を表すところに、この対角共分散を出すということが大事なのですけれども、これが正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布の中で、正規分布これは、総分線よりは、サウンド検察の方が難しいということで、混合分布みたいな選手番が使われます。これは、実際に混合分布を使って認識モデルを作ってあげると、性能的には、当然、これはデータの数によるけれども、この左のダイヤルなりよりは、いくらか良くなりそうな感じがします。元のデータにぴったり合わさっているということです。これ、割とよく使われてきたけれども、最近はどうなのかな、最近は、どちらかというと、この分布そのものを機械学習でモデル化しようと、いわゆる正規分布みたいなものを案に仮定しないことが増えてきているので、リアルネットワークなんてのは、まさにノンパラメトリックな分布の代表格ですよ。だから、最近のトレンドとしては、もう混合分布とかそういうところで工夫をする必要性も徐々になくなりつつあるのかなと思いますが、やっぱり、これは認識の確立の考え方の基礎としては大事な話なので、なんとなくこういうアイデアがあるよというのは知っておくべきだと思います。はい、それでもう1本は無理ですね。あとですね、あと残りはちょっとこのHMMに関するいろんな工夫が行われますよという話と、あとちょっと手前にその話も入れてあるはずですが、私が昔取り組んできた研究をいろいろと紹介した上で、HMMの話は終わりにしようかなと思います。はい、来週までだったと思います。げきまちくの課題の締め切りで、さっきちらっと見たらすでに23人くらいの人が提出してくれている事態です。中はまだ見てないかといいますが、頑張って取り組んでください。何とか言っているように、課題はそれなりの比重を持って、まず試験相当の評価をしたいと思いますので、それなりに頑張ってくれた人には良い評価がつけられると思います。それで、次回あたりにもう一度アナウンスしたいかな。今日はまだ誰事課題の前ですから、今まで言う事ではないけれども、皆さんの来週のペース状況の内容を見た上で、その先については考えようと思います。とりあえず、課題を頑張ってやってくださいということで、今日はこの状況を終わりにしております。最初から言ってくれた人は、今日の小レポとかでよろしいでしょうか。しっかり書いておければと思います。はい、以上で終わります。何か言い忘れた?いやいや、レポートやってください。はい、終わりにします。今日のこの6個の説明がありますよ。ありますよ。